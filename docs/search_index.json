[
["index.html", "Final Report Preface", " Final Report Luke Le 2021-11-12 Preface A summary note of Time Series Fundamentals https://bookdown.org/gary_a_napier/time_series_lecture_notes/ChapterOne.html "],
["chapter-1-time-series-fundamentals.html", "1 Chapter 1: Time Series Fundamentals 1.1 Definition 1.2 White Noise: purely random process 1.3 Random Walk: random but dependent 1.4 Time Series Modelling 1.5 Time Series Properties 1.6 Stationarity", " 1 Chapter 1: Time Series Fundamentals 1.1 Definition \\[\\{X_t|t\\in T \\}\\] where \\(X_t\\) denotes random variables that are continuous, \\(T\\) denotes index sets that are discrete and equally spaced in time \\(x_t\\) denotes observations, or realisations of \\(X_t\\) 1.2 White Noise: purely random process \\[E[X_t]=\\mu\\] \\(Var[X_t]=\\sigma^2\\) where each \\(X_t\\) is independent It asssumes the observations are all independent 1.3 Random Walk: random but dependent \\[ X_T = X_{t-1} +Z_t\\] where \\(Z_t\\) is a purely random process with mean \\(\\mu\\) and variance \\(\\sigma^2\\) 1.4 Time Series Modelling Time series data are often decomposed into the following three components: Trend, Seasonal Effect, Unexplained variation Data with additive structure is easier to analyze as compared to multiplicative structure. If the time series data has multiplicative structure, we can model it by using transformations. 1.4.1 Log Transformation \\[log(X_t)=log(m_t\\cdot s_t\\cdot e_t)=log(m_t)+log(s_t)+log(e_t)\\] Use to stablize variance, make seasonal effect \\(s_t\\) additive, &amp; make the data normally distributed. 1.4.2 Box-Cox Transformation \\[ y_t=(x_t^\\lambda-1)/\\lambda \\space\\space \\lambda\\ne0\\] \\[ or \\space y_t=ln(x_t) \\space\\space \\lambda=0\\] where \\(\\lambda\\) is a tuning parameter chosen by the analyst 1.5 Time Series Properties 1.5.1 Mean function (if mean is constant) \\[\\mu_t=E[X_t]\\] \\[\\hat \\mu=\\frac 1n\\sum_{t=1}^{n}x_t\\] In case of walking average, see ch.2 1.5.2 Variance function (if variance is constant) \\[\\sigma_t^2=Var[X_t]=E[X_t^2]-E[X_t]^2\\] \\[\\hat \\sigma^2=\\frac1{n-1} \\sum_{t=1}^n{(x_t-\\hat\\mu)^2}\\] 1.5.3 Autocovariance function (ACVF): \\[\\gamma_{s,t}=Cov[X_s,X_t]=E[X_sX_t]-E[X_t]E[X_s]\\] where \\(\\gamma_{t,t}=Cov[X_t,X_t]=Var[X_t]=\\sigma_t^2\\) Real data: \\[\\gamma_\\tau=Cov[X_tX_{t+\\tau}]\\] with lag \\(\\tau=0,1,2,..\\) 1.5.4 Autocorrelation function (ACF): \\[\\rho_{s,t}=Corr[X_s,X_t]=\\frac{Cov[X_s,X_t]}{\\sqrt{Var[X_s]Var[X_t]}}=\\frac{\\gamma_{s,t}}{\\sigma_s\\sigma_t}\\] where \\(\\rho_{t,t}=Corr[X_t,X_t]=1\\) Real data: \\[\\rho_\\tau=Corr[X_tX_{t+\\tau}]=\\frac{Cov[X_t,X_{t+\\tau}]}{\\sqrt{Var[X_t]Var[X_{t+\\tau}]}}=\\frac{\\gamma_\\tau}{\\gamma_0}\\] 1.5.5 Properties Property 1: \\(\\rho_\\tau=\\rho_{-\\tau}\\) Property 2: \\(|\\rho_\\tau|\\leq1\\) Property 3: Invertibility is not assumed 1.6 Stationarity A time series process \\(\\{X_t|t\\in T\\}\\) is strictly stationary if the joint distribution \\(f(X_{t1},...,X_{tk})\\) is identical to the joint distribution \\(f(X_{t1+r},...,X_{tk+r})\\) for all collections \\(t_1,...,t_k\\) and separation values \\(r\\). In other words, shifting the time origin of the series by \\(r\\) has no effect on its joint distribution. A time series process \\(\\{X_t|t\\in T\\}\\) is weakly stationary (or second-order stationary) if 1, mean function is constant and finite; 2, variance function is constant and finite; 3, autocovariance and autocorrelation functions only depend on the lag. "],
["chapter-1-lab.html", "2 Chapter 1 Lab 2.1 White Noise (WN) 2.2 Random Walk (RW) 2.3 Stationary", " 2 Chapter 1 Lab Using R to dive in the building blocks of time series models. These building blocks include White Noise (WN), Random Walk (RW), Correlation analysis and ACF/PACF graphs, Autoregressive model (AR), and Moving Average (MA). First thing to note is that sampling frequency tend to be not exact. If the data was recorded daily or hourly, that is exact sampling frequency. But if data is recorded monthly or yearly, it is not since months differ in number of days. Second thing to know is detrending. Detrending is necessary to remove variability resulted from “trend” over time (oftentimes using diff()). Think of this as standardized for analysis. Sometimes the data needs log transformation before detrending (more on this in future chapter) 2.1 White Noise (WN) Introduce the simulation function: WN_example &lt;- arima.sim(model = list(order = c(0,0,0)), n = 50) ts.plot(WN_example) For now, just know that the c(0,0,0) specifies the WN model. Note that WN model has no pattern or trend. Named after white light in physics which display similar characteristics. 2.2 Random Walk (RW) \\[ X_T = X_{t-1} +Noise_t\\] with \\(Noise_t = Z_t (White noise)\\) Think of White Noise but with trend. The value of the next period depends on the value of the previous with a random level of noise. random_walk &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 100) plot(random_walk) Again, just know that 0,1,0 specifies RW. 2.3 Stationary Stationary is preferable since its behavior can be modeled with fewer parameter (which we will get into in ARIMA). Examples: library(quantmod) getSymbols(&quot;CPIAUCSL&quot;, auto.assign = TRUE, src = &quot;FRED&quot;) ## [1] &quot;CPIAUCSL&quot; # Here I imported the CPI data from the FRED database. CPI is nonstationary, its difference (which is inflation rate) is nonstationary, but inflation&#39;s difference is stationary plot(CPIAUCSL) plot(diff(CPIAUCSL)) plot(diff(diff(CPIAUCSL))) "],
["chapter-2-modelling-trends-and-seasonal-patterns.html", "3 Chapter 2: Modelling trends and seasonal patterns 3.1 Method 1: Regression 3.2 Method 2: Moving Average Smoothing 3.3 Method 3: Differencing 3.4 Choosing a smoothing parameter", " 3 Chapter 2: Modelling trends and seasonal patterns Trend and Seasonality can be the main interests Need to remove trend and seasonality to determine short-term correlation For this chapter, assume additivity: \\(X_t=m_t+s_t+e_t\\) (Refer to Ch.1) First is to estimate the trend and seasonal variation \\(\\hat m_t+\\hat s_t\\), then calculate the residual series \\(e_t^*=X_t-\\hat m_t-\\hat s_t\\) 3 of the most common methods for modelling trend and seasonality are described here. 3.1 Method 1: Regression \\[m_t+s_t=\\beta_0+\\beta_1z_{t1}+...+\\beta_pz_{tp}\\] Cons: OLS estimation assumes observations are independent. Pros: Remove trend to later model the correlation by a stationary time series process \\(z_{tn}\\) are often functions of time 3.1.1 Examples 3.1.1.1 Linear trend in time but no seasonal variation: Trend can be modelled by \\(X_t=\\beta_0+\\beta_1t+e_t\\) and using series \\(e_t\\) to analyze short-term correlation 3.1.1.2 Quadratic trend in time but no seasonal variation: Trend can be modeled by \\(X_t=\\beta_0+\\beta_1t+\\beta_2t^2+e_t\\) 3.1.1.3 Seasonal pattern in time and a linear trend with 365 data points: Trend and Seasonality can be modeled by \\(X_t=\\beta_0+\\beta_1t+\\beta_2sin(8\\pi t/365)+e_t\\) 3.1.2 Other common models 3.1.2.1 Other Covariates \\[m_t=\\beta+0+\\beta_1\\alpha_t\\] 3.1.2.2 Polynomials \\[m_t=\\beta+0+\\beta_1t+\\beta_qt^q\\] Higher \\(q\\), more flexible the trend 3.1.2.3 Harmonics \\[s_t=\\beta_0+\\sum_{i=1}^q\\beta_{1i}sin(w_it)+\\beta_{2i}cos(w_it)\\] Harmonics assume the seasonal pattern has a regular shape, i.e. the height of the peaks is the same as the depth of the troughs. 3.1.2.4 Seasonal Factors Assuming the seasonal pattern repeats itself every d time points, a less restrictive approach is to model it as: \\[s_{t}=\\left\\{\\begin{array}{cc}0&amp;\\mbox{if}~t=1, d+1, 2d+2,\\ldots\\\\ s_{2}&amp;\\mbox{if}~t=2, d+2, 2d+2,\\ldots\\\\ \\vdots&amp;\\vdots\\\\ s_{d}&amp;\\mbox{if}~t=d, 2d, 3d,\\ldots\\\\\\end{array}\\right.\\] This model can be fitted by creating \\(d - 1\\) dummy variables in the design matrix, that contain 1’s and 0’s. 3.1.2.5 Natural cubic splines (More flexible) \\[m_{t}+s_{t}=\\beta_{0}+\\sum_{i=1}^{q}\\beta_{i}B_{i}(t)\\] 3.2 Method 2: Moving Average Smoothing A Moving average smoother estimates the trend and seasonal variation at time t by averaging the current observation and the q either side \\[\\hat{m}_{t}+\\hat{s}_{t}=\\frac{1}{2q+1}\\sum_{j=-q}^{q}x_{t-j}\\] Note: Shorten length of time series by \\(2q\\), therefore if the trend is smooth and \\(q\\) is large, the series shortens significantly. 3.3 Method 3: Differencing 3.3.1 Remove Trends 3.3.1.1 First order difference operator \\(\\nabla\\): \\[\\nabla X_{t}=X_{t}-X_{t-1}=(1-B)X_{t}\\] where \\(B\\) is the Backshift operator defined as \\(BX_{t}=X_{t-1}\\) 3.3.1.2 General order difference operator \\(\\nabla^q\\) \\[\\nabla^{q} X_{t}=\\nabla[\\nabla^{q-1}X_{t}]\\] \\[B^{q}X_{t}=X_{t-q}\\] Notes: 1. A polynomial trend of order \\(q\\) can be removed by \\(q^{th}\\) order differencing 2. Typically only first or second order differencing is required 3. Shortens length by \\(q\\) 4. Differencing won’t allow to estimate the trend but only to remove it. 3.3.2 Remove Seasonality The seasonal difference of order \\(d\\) is the operator \\(\\nabla_d\\) given by \\[\\nabla_{d} X_{t}=X_{t}-X_{t-d}=(1-B^{d})X_{t}\\] Notes: 1. Trial and Error approach 2. No point differencing twice if once is adequate 3. Differencing increases the variance 3.4 Choosing a smoothing parameter 3.4.1 Simplicity Simple statistical models are preferred since they are easier to make inferences from. 3.4.2 Objective criteria (AIC &amp; BIC) Assumptions: 1. Normally distributed observations 2. \\(m_{t}+s_{t}=\\textbf{z}_{t}^\\top \\boldsymbol{\\beta}\\) where \\(\\textbf{z}_{t}^\\top\\) is a vector of \\(q-1\\) known covariates and a one to represent the intercept term 3.4.2.1 Akaike’s Information Criterion (AIC) \\[\\mbox{AIC}(q)=-2\\mathcal{L}(\\mathbf{x}|\\hat{\\boldsymbol{\\beta}}) + 2q\\] 3.4.2.2 Bayesian Information Criterion (BIC) \\[\\mbox{BIC}(q)=-2\\mathcal{L}(\\mathbf{x}|\\hat{\\boldsymbol{\\beta}}) + \\ln(n)q\\] where \\(\\mathcal{L}(\\mathbf{x}|\\hat{\\boldsymbol{\\beta}})\\) is the maximised log likelihood function of \\(\\mathbf{x}\\) Both criteria tradeoff the fit to the data against simplicity (i.e. few parameters), and small values suggest a good fit to the data. "],
["chapter-2-lab.html", "4 Chapter 2 Lab 4.1 Model choice and Residual analysis", " 4 Chapter 2 Lab 4.1 Model choice and Residual analysis AIC and BIC are great measures for fit of models: \\[ average(observed-predicted)^2+k(p+q)\\] The smaller these measures are, the better the prediction. As mentioned, sarima() returns 4 graphs. Given we want the model to predict most of the observed data, and the residuals are just white noise, these 4 tools can help us identify whether residuals are just white noise: Standardized residuals: does it look like white noise? Sample ACF of residuals: no statistically significant ACF (everything within the blue lines) Normal Q-Q plot: it should look like a line with a few outliers Q-statistic p-values: all p-values should be above the line Finding the right model most of the time are through trials and errors starting with some ideas based off what we know about the data. library(astsa) dl_varve &lt;- diff(log(varve)) # Fit an MA(1) to dl_varve. sarima(dl_varve, p = 0, d = 0, q = 1) ## initial value -0.551780 ## iter 2 value -0.671633 ## iter 3 value -0.706234 ## iter 4 value -0.707586 ## iter 5 value -0.718543 ## iter 6 value -0.719692 ## iter 7 value -0.721967 ## iter 8 value -0.722970 ## iter 9 value -0.723231 ## iter 10 value -0.723247 ## iter 11 value -0.723248 ## iter 12 value -0.723248 ## iter 12 value -0.723248 ## iter 12 value -0.723248 ## final value -0.723248 ## converged ## initial value -0.722762 ## iter 2 value -0.722764 ## iter 3 value -0.722764 ## iter 4 value -0.722765 ## iter 4 value -0.722765 ## iter 4 value -0.722765 ## final value -0.722765 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 xmean ## -0.7710 -0.0013 ## s.e. 0.0341 0.0044 ## ## sigma^2 estimated as 0.2353: log likelihood = -440.68, aic = 887.36 ## ## $degrees_of_freedom ## [1] 631 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.7710 0.0341 -22.6002 0.0000 ## xmean -0.0013 0.0044 -0.2818 0.7782 ## ## $AIC ## [1] 1.401826 ## ## $AICc ## [1] 1.401856 ## ## $BIC ## [1] 1.422918 # Fit an MA(2) to dl_varve. Improvement? sarima(dl_varve, p = 0, d = 0, q = 2) ## initial value -0.551780 ## iter 2 value -0.679736 ## iter 3 value -0.728605 ## iter 4 value -0.734640 ## iter 5 value -0.735449 ## iter 6 value -0.735979 ## iter 7 value -0.736015 ## iter 8 value -0.736059 ## iter 9 value -0.736060 ## iter 10 value -0.736060 ## iter 11 value -0.736061 ## iter 12 value -0.736061 ## iter 12 value -0.736061 ## iter 12 value -0.736061 ## final value -0.736061 ## converged ## initial value -0.735372 ## iter 2 value -0.735378 ## iter 3 value -0.735379 ## iter 4 value -0.735379 ## iter 4 value -0.735379 ## iter 4 value -0.735379 ## final value -0.735379 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 ma2 xmean ## -0.6710 -0.1595 -0.0013 ## s.e. 0.0375 0.0392 0.0033 ## ## sigma^2 estimated as 0.2294: log likelihood = -432.69, aic = 873.39 ## ## $degrees_of_freedom ## [1] 630 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.6710 0.0375 -17.9057 0.0000 ## ma2 -0.1595 0.0392 -4.0667 0.0001 ## xmean -0.0013 0.0033 -0.4007 0.6888 ## ## $AIC ## [1] 1.379757 ## ## $AICc ## [1] 1.379817 ## ## $BIC ## [1] 1.40788 # Fit an ARMA(1,1) to dl_varve. Improvement? sarima(dl_varve, p = 1, d = 0, q = 1) ## initial value -0.550994 ## iter 2 value -0.648962 ## iter 3 value -0.676965 ## iter 4 value -0.699167 ## iter 5 value -0.724554 ## iter 6 value -0.726719 ## iter 7 value -0.729066 ## iter 8 value -0.731976 ## iter 9 value -0.734235 ## iter 10 value -0.735969 ## iter 11 value -0.736410 ## iter 12 value -0.737045 ## iter 13 value -0.737600 ## iter 14 value -0.737641 ## iter 15 value -0.737643 ## iter 16 value -0.737643 ## iter 17 value -0.737643 ## iter 18 value -0.737643 ## iter 18 value -0.737643 ## iter 18 value -0.737643 ## final value -0.737643 ## converged ## initial value -0.737522 ## iter 2 value -0.737527 ## iter 3 value -0.737528 ## iter 4 value -0.737529 ## iter 5 value -0.737530 ## iter 5 value -0.737530 ## iter 5 value -0.737530 ## final value -0.737530 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 xmean ## 0.2341 -0.8871 -0.0013 ## s.e. 0.0518 0.0292 0.0028 ## ## sigma^2 estimated as 0.2284: log likelihood = -431.33, aic = 870.66 ## ## $degrees_of_freedom ## [1] 630 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2341 0.0518 4.5184 0.0000 ## ma1 -0.8871 0.0292 -30.4107 0.0000 ## xmean -0.0013 0.0028 -0.4618 0.6444 ## ## $AIC ## [1] 1.375456 ## ## $AICc ## [1] 1.375517 ## ## $BIC ## [1] 1.403579 "],
["chapter-3-autoregressive-processes.html", "5 Chapter 3: Autoregressive processes 5.1 Definition 5.2 First order Autoregressive process 5.3 \\(AR(p)\\) process 5.4 When and how to use AR(p) model", " 5 Chapter 3: Autoregressive processes \\(Z_t\\) is purely random with mean zero and variance \\(\\sigma_z^2\\) 5.1 Definition An Autoregressive process of order \\(p\\), denoted \\(AR(p)\\), is given by \\[X_{t}=\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t}\\] Where we assume \\(X_{0}=X_{-1}=\\ldots=X_{1-p}=0\\) Essentially regress \\(X_t\\) to its own past values 5.2 First order Autoregressive process An \\(AR(1)\\) process is given by \\[X_{t}=\\alpha X_{t-1}+Z_{t}\\] 5.2.1 Mean \\[E(AR(1))=0\\] 5.2.2 Variance If \\(|\\alpha|\\geq1\\) then \\(Var[X_t]=\\infty\\) If \\(|\\alpha|\\lt1\\) then \\(\\alpha^2\\lt1\\), and: \\[\\sum_{j=0}^{\\infty}\\alpha^{2j}=1+\\alpha^{2}+\\alpha^{4}+\\alpha^{6}+\\ldots~=~\\frac{1}{1-\\alpha^{2}}\\] So we have: \\[\\mathrm{Var}[X_{t}]=\\sigma_{z}^{2}\\sum_{j=0}^{\\infty}\\alpha^{2j}=\\frac{\\sigma_{z}^{2}}{1-\\alpha^{2}} \\] 5.2.3 Examples 5.3 \\(AR(p)\\) process is given by \\[X_{t}=\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t} \\] \\(Z_t\\) can be written as: \\[\\begin{eqnarray} X_{t}&amp;=&amp;\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t}\\nonumber\\\\ X_{t}-\\alpha_{1}X_{t-1}-\\ldots-\\alpha_{p}X_{t-p}&amp;=&amp;Z_{t}\\nonumber\\\\ (1-\\alpha_{1}B-\\alpha_{2}B^{2}-\\ldots-\\alpha_{p}B^{p})X_{t}&amp;=&amp;Z_{t}\\nonumber\\\\ \\phi(B)X_{t}&amp;=&amp;Z_{t}\\nonumber \\end{eqnarray} \\] where \\(B\\) is the backshift operator 5.3.1 Mean \\[E[AP(p)] = 0\\] 5.3.2 Stationarity We calculate the variance and autocorrelation function conditional on the process being stationary. The following theorem tells us when an AR(p) process is stationary. Theorem AR(p) written as \\(\\phi(B)X_{t}=Z_{t}\\) where \\(\\phi(B)\\) is the characteristic polynomial \\(\\phi(B)=1-\\alpha_{1}B-\\alpha_{2}B^{2}-\\ldots-\\alpha_{p}B^{p}\\) Then the process is stationary if the roots of the characteristic equation \\[\\phi(B)=1-\\alpha_{1}B-\\alpha_{2}B^{2}-\\ldots-\\alpha_{p}B^{p}=0\\] have modulus greater than 1, i.e. they lie outside the unit circle. Here we consider \\(B\\) as the variable of the polynominal equation 5.3.3 Variance \\[\\mathrm{Var}[X_{t}]=\\sigma^{2}_{z}+\\sum_{\\tau=1}^{p}\\alpha_{\\tau}\\gamma_{\\tau} \\] 5.3.4 Autocorrelation Function Using Yule-Walker equation: \\[\\rho_{\\tau}=\\alpha_{1}\\rho_{\\tau-1}+\\ldots+\\alpha_{p}\\rho_{\\tau-p}\\] Examples for \\(\\tau = 1\\) and \\(\\tau = 2\\): \\[\\rho_{1}=\\frac{\\alpha_{1}}{1-\\alpha_{2}}.\\] \\[\\rho_{2}=\\alpha_{1}\\rho_{1}+\\alpha_{2}\\rho_{0}=\\frac{\\alpha_{1}^{2}}{1-\\alpha_{2}}+\\alpha_{2}\\] Note: as \\(\\tau\\) increases the function gets more complex algebraically. Same thing with p increases in AR(p) 5.4 When and how to use AR(p) model Realisations from the following AR(1) and AR(2) models: \\[X_t=0.6X_{t-1}+Z_t\\] \\[X_t=0.75X_{t-1}-0.125X_{t-2}+Z_t\\] # Simulate AR(1) and AR(2) data n &lt;- 100 sd &lt;- 10 data1 &lt;- arima.sim(model = list(ar = 0.6), n = n, sd = sd) data2 &lt;- arima.sim(model = list(ar = c(0.75, -0.125)), n = n, sd = sd) # Plot the correlogram and time plot par(mfrow = c(2, 2)) plot(data1, main = &quot;AR(1) data&quot;) acf(data1, main = &quot;AR(1) data&quot;) plot(data2, main = &quot;AR(2) data&quot;) acf(data2, main = &quot;AR(2) data&quot;) The ACF are the same. It will be better to look at the correlogram of the residuals to see if the residuals resemble a purely random process. If there is still residual correlation, increase \\(p\\) by one and re-fit the model and repeat. However, this is not a good method if AR(p) isn’t a suitable model at the first place. 5.4.1 Definition of PACF The partial autocorrelation function (PACF) at lag \\(τ\\) is equal to the estimated lag \\(τ\\) coefficient \\(\\hat \\alpha_{\\tau}\\), obtained when fitting an AR(\\(τ\\)) model to a data set. It is denoted by \\(π_τ\\), and represents the excess correlation in the time series that has not been accounted for by the \\(τ-1\\) smaller lags. The partial autocorrelation function allows us to determine an appropriate AR(p) process for a given data set. This is because if the data do come from an AR(p) process then: \\[\\pi_{\\tau} = \\left\\{\\begin{array}{cc}\\mbox{positive}&amp;\\tau\\leq p\\\\0&amp;\\tau&gt;p\\end{array}\\right.\\] This is because by definition, PACF is only measuring the direct correlation of the data with lag \\(\\tau\\). The AR(p) model does not factor in lag bigger than p, making the coefficient of those would be 0. Therefore, to choose the order of an AR(p) process, plot the PACF and choose p as the smallest value that is significantly different from zero. 5.4.1.1 Example Looking at PACF, Data 1 uses AR(1) process, Data 2 uses AR(2), Data 3 uses AR(3), Data 4 uses AR(12) "],
["chapter-3-lab.html", "6 Chapter 3 Lab 6.1 Autoregressive model (AR)", " 6 Chapter 3 Lab 6.1 Autoregressive model (AR) AR(p) process is correlation with lagged values of the data itself. \\[ X_t-\\mu = \\phi \\cdot (X_{t-1}-\\mu)+Noise_t\\] Some simulation examples down below. Note how the coefficent \\(\\phi\\) affects the look of the graph: x &lt;- arima.sim(model = list(ar = 0.5), n = 100) # Simulate an AR model with 0.9 slope y &lt;- arima.sim(model = list(ar = 0.9), n = 100) # Simulate an AR model with -0.75 slope z &lt;- arima.sim(model = list(ar = -0.75), n = 100) par(mfrow = c(3,1)) # Plot your simulated data plot.ts(cbind(x, y, z)) acf(x) acf(y) acf(z) "],
["chapter-4-moving-average-processes.html", "7 Chapter 4: Moving Average processes 7.1 Definition 7.2 Mean and Variance 7.3 Autocorrelation functions 7.4 Invertibility 7.5 MA Model Identification 7.6 MA parameter estimation 7.7 MA Example", " 7 Chapter 4: Moving Average processes 7.1 Definition A Moving Average process of order \\(q\\), denoted MA(\\(q\\)), is given by \\[X_{t}=\\lambda_{0}Z_{t}+\\lambda_{1}Z_{t-1}+\\ldots+\\lambda_{q}Z_{t-q}\\] where \\(\\lambda_0=1\\) 7.2 Mean and Variance Mean of any MA(\\(q\\)) process: \\(E[X_t]=0\\) Variance: \\[X_{t}=\\sigma^{2}_{z}\\left[1+\\sum_{j=1}^{q}\\lambda_{j}^{2}\\right]\\] 7.3 Autocorrelation functions Autocovariance function: \\[\\gamma_{\\tau}=\\mathrm{Cov}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}\\sigma_{z}^{2}\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}&amp;\\mbox{if}~\\tau=0,1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right.\\] and autocorrelation function: \\[\\rho_{\\tau}=\\mathrm{Corr}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}}{\\sum_{j=0}^{q}\\lambda_{j}^{2}}&amp;\\mbox{if}~\\tau=1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right.\\] where \\(\\lambda_0=1\\) Notes: The mean and variance of any MA(\\(q\\)) process are finite and constant, while the autocorrelation function is finite and does not depend on \\(t\\). Therefore any MA(\\(q\\)) is weakly stationary. The autocorrelation function of an MA(\\(q\\)) process is positive at lags \\(1,…,q\\) and zero for any lag greater than \\(q\\). This gives us a method for detecting whether an MA(\\(q\\)) process is an appropriate model for a given data set. 7.3.1 Examples: Consider the MA(1) process: \\(X_t = Z_t + \\lambda Z_{t-1}\\). Its variance is given by \\[\\begin{eqnarray} \\mathrm{Var}[X_{t}]&amp;=&amp;\\mathrm{Var}[Z_{t}+\\lambda Z_{t-1}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}(1+\\lambda^{2}).\\nonumber \\end{eqnarray}\\] Its lag one autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+1}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+\\lambda Z_{t-1},~Z_{t+1}+\\lambda Z_{t}]\\nonumber\\\\ &amp;=&amp;\\mathrm{Cov}[Z_{t},~Z_{t+1}]+\\lambda \\mathrm{Cov}[Z_{t},~Z_{t}]+\\lambda \\mathrm{Cov}[Z_{t-1},~Z_{t+1}]+\\lambda^{2}\\mathrm{Cov}[Z_{t-1},~Z_{t}]\\nonumber\\\\ &amp;=&amp;\\lambda \\sigma_{z}^{2}\\nonumber \\end{eqnarray}\\] lag 2 autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+2}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+\\lambda Z_{t-1},~Z_{t+2}+\\lambda Z_{t+1}]\\nonumber\\\\ &amp;=&amp;\\mathrm{Cov}[Z_{t},~Z_{t+2}]+\\lambda \\mathrm{Cov}[Z_{t},~Z_{t+1}]+\\lambda \\mathrm{Cov}[Z_{t-1},~Z_{t+2}]+\\lambda^{2}\\mathrm{Cov}[Z_{t-1},~Z_{t+1}]\\nonumber\\\\ &amp;=&amp;0.\\nonumber \\end{eqnarray}\\] the autocorrelation function is given by \\[\\rho_{\\tau}=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\lambda}{1+\\lambda^{2}}&amp;\\mbox{if}~\\tau=1\\\\ 0&amp;\\tau&gt;1\\\\ \\end{array}\\right.\\] 7.3.2 Another example Consider the MA(2) process \\(X_t = Z_t +0.9Z_{t-1}+0.5Z_{t-2}\\). Its variance is given by \\[\\begin{eqnarray} \\mathrm{Var}[X_{t}]&amp;=&amp;\\mathrm{Var}[Z_{t}+0.9 Z_{t-1} + 0.5Z_{t-2}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}(1+0.81 + 0.25)\\nonumber\\\\ &amp;=&amp;2.06\\sigma^{2}_{z}.\\nonumber \\end{eqnarray} \\] Its lag one autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+1}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+0.9 Z_{t-1}+0.5Z_{t-2},~Z_{t+1}+0.9 Z_{t} +0.5Z_{t-1}]\\nonumber\\\\ &amp;=&amp;0.9 \\mathrm{Cov}[Z_{t},~Z_{t}]+0.45 \\mathrm{Cov}[Z_{t-1},~Z_{t-1}]\\nonumber\\\\ &amp;=&amp;1.35\\sigma_{z}^{2}\\nonumber \\end{eqnarray} \\] Its lag 2 autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+2}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+0.9 Z_{t-1}+ 0.5Z_{t-2},~Z_{t+2}+0.9 Z_{t+1}+ 0.5Z_{t}]\\nonumber\\\\ &amp;=&amp;0.5\\mathrm{Cov}[Z_{t},~Z_{t}]\\nonumber\\\\ &amp;=&amp;0.5\\sigma^{2}_{z}.\\nonumber \\end{eqnarray} \\] Therefore the autocovariance function is given by \\[\\gamma_{\\tau}=\\left\\{\\begin{array}{cc}2.06\\sigma^{2}_{z}&amp;\\mbox{if}~\\tau=0\\\\ 1.35\\sigma^{2}_{z}&amp;\\mbox{if}~\\tau=1\\\\ 0.5\\sigma^{2}_{z}&amp;\\mbox{if}~\\tau=2\\\\ 0&amp;\\tau&gt;2\\\\ \\end{array}\\right. \\] while the autocorrelation function is given by \\[\\rho_{\\tau}=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ 0.655&amp;\\mbox{if}~\\tau=1\\\\ 0.243&amp;\\mbox{if}~\\tau=2\\\\ 0&amp;\\tau&gt;2\\\\ \\end{array}\\right. \\] 7.4 Invertibility Therefore, given a set of sample autocorrelation functions \\(\\hat{\\boldsymbol{\\rho}}_p\\) calculated from a time series, there is a unique set of parameters $that best fit the data within the class of stationary AR(p) processes. For an MA(q) process the corresponding relationship is given by \\[\\rho_{\\tau}=\\mathrm{Corr}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}}{\\sum_{j=0}^{q}\\lambda_{j}^{2}}&amp;\\mbox{if}~\\tau=1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right. \\] which is a set of non-linear equations in \\(\\boldsymbol{\\lambda}=(\\lambda_{1},\\ldots,\\lambda_{q})\\). Therefore for a given autocorrelation function \\(\\boldsymbol{\\rho}_{q}\\) there may exist more than one set of parameters \\(\\lambda\\) that satisfy the above equation. 7.4.1 Inveritibility Theorem The MA(q) process \\[\\begin{eqnarray} X_{t}&amp;=&amp;Z_{t}+\\lambda_{1}Z_{t-1}+\\ldots+\\lambda_{q}Z_{t-q}\\nonumber\\\\ &amp;=&amp;(1+\\lambda_{1}B+\\ldots+\\lambda_{q}B^{q})Z_{t}\\nonumber\\nonumber\\\\ &amp;=&amp;\\theta(B)Z_{t}\\nonumber \\end{eqnarray} \\] is invertible if and only if the roots of the characteristic polynomial \\(\\theta (B)\\) have modulus greater than one and hence lie outside the unit circle. 7.4.1.1 Example of invertibility \\[X_{t}=Z_{t} + 4.25Z_{t-1} + Z_{t-2}\\] Characteristic polynomial is given by \\(\\theta(B)~=~1 + 4.25B + B^{2}\\) Solving \\(1 + 4.25B + B^{2}=0\\) gives: \\[\\begin{eqnarray} \\mbox{roots}&amp;=&amp;\\frac{-4.25\\pm\\sqrt{(4.25)^2-4\\times1\\times1}}{2\\times 1}\\nonumber\\\\ \\mbox{roots}&amp;=&amp;\\frac{-4.25\\pm3.75}{2}\\nonumber\\\\ \\mbox{roots}&amp;=&amp;-0.25\\mbox{ and }-4\\nonumber \\end{eqnarray} \\] Therefore as one of the routes has modulus less than one the process is not invertible. 7.5 MA Model Identification Determine if a MA model is appropriate and which order process should we use using the autocorrelation function ACF: \\[\\rho_{\\tau}=\\mathrm{Corr}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}}{\\sum_{j=0}^{q}\\lambda_{j}^{2}}&amp;\\mbox{if}~\\tau=1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right. \\] This function is non-zero for \\(\\tau \\le q\\) and zero for \\(\\tau &gt;q\\) Note The autocorrelation function (ACF) tells us whether an MA(q) process is appropriate while the partial autocorrelation function (PACF) suggests whether an AR(p) process is appropriate. Therefore for a given time series, both should be plotted to show which process would be a good model. 7.5.1 Example Looking at ACF, Data 1 uses MA(1) process, Data 2 uses MA(2), Data 3 uses MA(3), Data 4 uses MA(12) 7.6 MA parameter estimation Conditional least squares: Consider the MA(1) model \\(X_{t}=\\mu+Z_{t}+\\lambda Z_{t-1}\\), where \\(\\mathrm{Var}[Z_{t}]=\\sigma^{2}_{z}\\). The conditional least squares algorithm works as follows: Select starting values for \\(\\mu\\) and \\(\\lambda\\), \\[\\tilde{\\mu}=\\frac{1}{n}\\sum_{t=1}^{n}x_{t}\\hspace{1cm}\\mbox{and by solving}\\hspace{1cm}\\hat{\\rho}_{1}=\\frac{\\tilde{\\lambda}}{1+\\tilde{\\lambda}^{2}} \\] Calculate the conditional residual sum of squares \\[S(\\tilde{\\mu},\\tilde{\\lambda})=\\sum_{t=1}^{n}[x_{t}-\\tilde{\\mu}-\\tilde{\\lambda}Z_{t-1}]^{2} \\] Where \\(Z_0=0\\) and \\(_Z_t\\) is calculated recursively using \\[Z_{t}=x_{t}-\\tilde{\\mu}-\\tilde{\\lambda}Z_{t-1} \\] Repeat step 2 for a range of values of \\((\\mu, \\lambda)\\) that are close to the initial estimates in step 1. Then determine the estimates \\((\\hat{\\mu},\\hat{\\lambda})\\) as the values that mimimize \\(S(\\tilde{\\mu},\\tilde{\\lambda})\\) over all those values considered Using the fact that the variance of an MA(1) process is \\(\\mathrm{Var}[X_{t}]=\\sigma^{2}_{z}(1+\\lambda^{2})\\), we obtain that \\[\\hat{\\sigma}^{2}_{z}=\\frac{\\hat{\\sigma}^{2}}{(1+\\hat{\\lambda}^{2})} \\] where \\(\\hat \\sigma^2\\) is the overall variance of the process. 7.7 MA Example # Generate MA(3) data with a quadratic trend time &lt;- 1:200 n &lt;- 200 sd &lt;- 10 data.ar &lt;- arima.sim(model = list(ma = c(0.6, 0.3, 0.6)), n = n, sd = sd) data &lt;- data.ar + 30 + 0.05 * time + 0.004*time^2 par(mfrow = c(2, 1)) plot(time, data, type = &quot;l&quot;, main = &quot;Raw data plot&quot;) acf(data, main = &quot;Sample autocorrelation function&quot;) From looking at the time plot and correlogram the data appear to have a quadratic trend, which we remove before trying to model the correlation. Fitting \\(m_{t}=\\beta_{0}+\\beta_{1}t+\\beta_{2}t^{2}\\) gives the follwing residual series: # Remove the trend time2 &lt;- time^2 linear.model &lt;- lm(data ~ time+time2) residual.series &lt;- data - linear.model$fitted.values par(mfrow = c(3,1)) plot(residual.series, main = &quot;Residual series&quot;) acf(residual.series, main = &quot;ACF of Residual series&quot;) pacf(residual.series, main = &quot;PACF of Residual series&quot;) Time plot suggests stationary while ACF suggest an MA(3) process is appropriate. Note that the PACF doesn’t tell us a lot here. Finally we fit a MA(3) model using function arima() in R # Fit an MA(3) model to the data model.ma &lt;- arima(residual.series, order = c(0, 0, 3)) model.ma ## ## Call: ## arima(x = residual.series, order = c(0, 0, 3)) ## ## Coefficients: ## ma1 ma2 ma3 intercept ## 0.6091 0.3694 0.6603 0.1269 ## s.e. 0.0549 0.0689 0.0540 1.8975 ## ## sigma^2 estimated as 104.7: log likelihood = -750.09, aic = 1510.19 arima(x = residual.series, order = c(0, 0, 3)) ## ## Call: ## arima(x = residual.series, order = c(0, 0, 3)) ## ## Coefficients: ## ma1 ma2 ma3 intercept ## 0.6091 0.3694 0.6603 0.1269 ## s.e. 0.0549 0.0689 0.0540 1.8975 ## ## sigma^2 estimated as 104.7: log likelihood = -750.09, aic = 1510.19 par(mfrow = c(3, 1)) plot(model.ma$residuals, main = &quot;MA(3) Residual series&quot;) acf(model.ma$residuals, main = &quot;MA(3) ACF of Residual series&quot;) pacf(model.ma$residuals, main = &quot;MA(3) PACF of Residual series&quot;) Residuals look independent (resemble white noise), so the model below is appropriate \\[X_{t}=\\beta_{0}+\\beta_{1}t+\\beta_{2}t^{2}+\\lambda_{1}Z_{t-1}+\\lambda_{2}Z_{t-2}+\\lambda_{3}Z_{t-3}+Z_{t}. \\] "],
["chapter-4-lab.html", "8 Chapter 4 Lab 8.1 Moving Average model (MA)", " 8 Chapter 4 Lab 8.1 Moving Average model (MA) \\[ X_t = \\mu + Noise_t + \\theta \\cdot(Noise_{t-1})\\] Regression on today’s data using yesterday’s noise. # Generate MA model with slope 0.5 x &lt;- arima.sim(model = list(ma = 0.5), n = 100) # Generate MA model with slope 0.9 y &lt;- arima.sim(model = list(ma = 0.9), n = 100) # Generate MA model with slope -0.5 z &lt;- arima.sim(model = list(ma = -0.5), n = 100) # Plot all three models together plot.ts(cbind(x, y, z)) par(mfrow = c(3, 1)) # Calculate the ACF acf(x) acf(y) acf(z) "],
["chapter-5-more-general-time-series-processes.html", "9 Chapter 5: More general time series processes 9.1 ARMA model 9.2 ARIMA model", " 9 Chapter 5: More general time series processes For most data sets the AR and MA models will be adequate to represent short-term correlation, with autoregressive correlation occurring more often than moving average. However, occasionally you may meet data that are not well represented by either of these time series processes, an example of which is shown below. Example: Consider the following data which appear to be stationary but contain short-term correlation. The ACF and PACF suggest that neither an AR nor an MA process is appropriate, but as these are the only models we know, we fit them to the data to see how well they remove the short-term correlation. We chose the order (p and q) as the lowest values that removed the majority of the correlation, which resulted in an AR(6) model or an MA(5) model: Neither fits the data perfectly as the residual series isn’t white noise. Both also used high order processess (p=6 and q=5) which include a relatively large number of parameters. This emphasizes two points: Even if the correlation structure does not look like an AR(p) or an MA(q) process, fitting these models with large enough p and q will remove the majority of the correlation. Therefore it is better to model correlation with the wrong time series process than not to model it at all. However, AR(p) and MA(q) processes are not always appropriate models for short-term correlation 9.1 ARMA model An Autoregressive Moving Average process of order (p,q) denoted ARMA(p,q) is given by \\[\\begin{eqnarray} X_{t}&amp;=&amp;\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p} + Z_{t} + \\lambda_{1}Z_{t-1}+\\ldots+\\lambda_{q}Z_{t-q}\\nonumber\\\\ &amp;=&amp;\\sum_{j=1}^{p}\\alpha_{j}X_{t-j}+\\sum_{j=1}^{q}\\lambda_{j}Z_{t-j} + Z_{t}\\nonumber \\end{eqnarray} \\] Using Backshift operator the model can be rewritten as \\[\\phi(B)X_{t} = \\theta(B)Z_{t}\\] Example: The data in the first example can be modelled by an ARMA(1,1) process \\[X_{t}=\\alpha X_{t-1}+\\lambda Z_{t-1}+Z_{t} \\] 9.1.1 Mean of ARMA(p,q) process \\[E[X_t] = 0\\] 9.1.2 Variance and autocorrelation function Variance: \\[\\begin{eqnarray} \\mathrm{Var}[X_{t}]&amp;=&amp;\\alpha \\gamma_{1}+\\lambda (\\alpha\\sigma^{2}_{z} + \\lambda\\sigma^{2}_{z})+\\sigma^{2}_{z}\\nonumber\\\\ &amp;=&amp;\\alpha \\gamma_{1}+\\sigma^{2}_{z}(\\lambda^{2}+\\lambda\\alpha+1).\\nonumber \\end{eqnarray} \\] Autocorrelation function: \\[\\rho_{\\tau}=\\alpha \\rho_{\\tau-1}\\] 9.1.3 Stationarity and Invertibility We know that MA(q) process is always stationary and AR(p) process is always invertible. Therefore an ARMA(p,q) process is stationary if the AR(p) part is stationary, i.e., if the roots of the AR(p) characteristic polynomial have modulus larger than 1; and is invertible if the MA(q) part is invertible, i.e., if the roots of the MA(q) characteristic polynomial have modulus larger than 1 Example: Consider the ARMA(1,1) process \\[X_{t}=2X_{t-1}-0.4Z_{t-1}+Z_{t}\\] which can be re-written as \\[X_{t}(1-2B)=Z_{t}(1-0.4B)\\] AR part has root \\(B=0.5\\), therefore the process is not stationary. The MA part has root \\(B=2.5\\), therefore the process is invertible. 9.1.4 ARMA model identification AR and MA processes are straightforward to identify from ACF and PACF: If the ACF is significantly different from zero for only the first q lags (for small q), then an MA(q) model is appropriate. If the PACF is significantly different from zero for only the first lags (for small p), then an AR(p) model is appropriate. Take another look at the data in the first example: Neither the ACF or PACF give any clues as to the appropriate type of time series process. All they tell us is that it is not an AR(p) process or an MA(q) process Notes: Model identification for ARMA(p,q) process when p,q &gt; 0 is difficult First determine if the ACF and PACF resemble either an MA(q) or an AR(p) process If not then adopt a trial and error approach, starting with the simplest model (i.e., an ARMA(1,1)) and increasing the complexity until the correlation has been removed. 9.1.5 ARMA parameter estimation Not in this course. 9.2 ARIMA model "],
["chapter-5-lab.html", "10 Chapter 5 Lab", " 10 Chapter 5 Lab An example using ARMA(1,1) process: library(astsa) # Calculate approximate oil returns oil_returns &lt;- diff(log(oil)) # Plot oil_returns. Notice the outliers. plot(oil_returns) # Plot the P/ACF pair for oil_returns acf2(oil_returns) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## ACF 0.13 -0.07 0.13 -0.01 0.02 -0.03 -0.03 0.13 0.08 0.02 0.01 0 -0.02 ## PACF 0.13 -0.09 0.16 -0.06 0.05 -0.08 0.00 0.12 0.05 0.03 -0.02 0 -0.03 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## ACF 0.06 -0.05 -0.09 0.03 0.05 -0.05 -0.07 0.04 0.09 -0.05 -0.08 -0.07 ## PACF 0.09 -0.07 -0.06 0.01 0.04 -0.05 -0.05 0.05 0.06 -0.06 -0.05 -0.08 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## ACF 0.00 -0.11 -0.07 0.02 -0.02 -0.03 -0.05 -0.03 0.00 -0.09 -0.01 -0.04 ## PACF 0.02 -0.11 0.01 0.00 -0.01 -0.05 -0.04 0.02 0.02 -0.08 0.02 -0.04 ## [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## ACF -0.01 0.02 -0.01 -0.06 0.01 0.00 -0.01 0.04 0.01 0.05 0.07 -0.01 ## PACF 0.04 -0.01 -0.01 -0.05 0.03 -0.03 0.00 0.08 0.00 0.05 0.01 0.04 ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] ## ACF -0.03 0.01 -0.04 -0.04 -0.03 0 -0.01 -0.10 -0.01 -0.05 -0.04 -0.03 ## PACF -0.08 0.01 -0.07 0.00 -0.06 0 -0.06 -0.11 0.01 -0.09 -0.01 -0.04 ## [,62] [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] ## ACF 0.01 0.01 -0.01 -0.04 0.02 0 -0.01 -0.03 -0.02 -0.05 -0.01 -0.01 ## PACF 0.04 -0.01 0.00 -0.04 0.03 0 0.00 -0.04 -0.02 -0.04 0.00 -0.01 ## [,74] [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] ## ACF -0.02 0.01 0.02 0.04 -0.01 0.03 0.02 -0.04 -0.01 0.02 0.03 0.01 ## PACF 0.00 0.02 -0.01 0.04 -0.02 0.08 -0.03 -0.03 -0.03 0.03 -0.03 -0.02 ## [,86] [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] ## ACF 0.03 0.08 -0.04 -0.02 0.01 -0.04 0.05 0.07 -0.04 0.02 0.05 0.01 ## PACF 0.03 0.04 -0.09 -0.01 -0.02 -0.03 0.03 0.05 -0.11 0.02 -0.01 0.02 ## [,98] [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108] ## ACF 0.00 0.01 0.04 0.01 -0.03 -0.04 -0.01 0.02 0.01 0.01 0.06 ## PACF -0.03 0.06 0.01 -0.05 0.02 -0.03 0.01 0.00 0.04 -0.01 0.07 ## [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118] ## ACF 0.08 0.04 0.02 0.01 0.03 0.02 -0.02 -0.04 -0.01 0.04 ## PACF 0.04 0.04 0.00 0.05 -0.01 0.00 -0.04 -0.03 -0.03 0.02 ## [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128] ## ACF 0.05 -0.02 -0.02 0.03 0.01 -0.04 -0.08 0.02 0.00 -0.04 ## PACF 0.04 -0.01 -0.04 -0.01 0.03 -0.03 -0.07 0.00 -0.02 -0.04 ## [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138] ## ACF 0.01 0.02 0.01 0.02 0.00 -0.01 0.00 -0.03 -0.06 0.01 ## PACF 0.01 0.01 -0.01 0.02 0.05 0.02 0.01 0.02 -0.02 0.04 ## [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148] ## ACF -0.02 -0.02 0.02 -0.01 -0.03 0.00 0.00 -0.04 -0.01 -0.02 ## PACF 0.01 -0.03 -0.02 0.02 -0.01 0.02 -0.01 0.02 -0.02 -0.03 ## [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158] ## ACF -0.04 -0.04 0.01 0.01 0.04 0.03 0.01 0.05 0.01 -0.06 ## PACF -0.02 -0.01 0.02 -0.01 0.04 0.03 -0.04 0.03 0.00 -0.05 ## [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168] ## ACF 0.02 0.05 -0.02 0.05 0.00 -0.01 0 -0.01 -0.02 -0.01 ## PACF 0.02 0.03 0.00 0.03 0.01 0.00 0 0.02 0.01 -0.01 ## [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178] ## ACF 0.00 -0.03 -0.01 -0.02 -0.02 0.04 -0.01 -0.03 0.02 0.01 ## PACF 0.03 -0.03 0.00 -0.04 0.00 0.02 -0.03 -0.01 0.01 0.02 ## [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188] ## ACF -0.01 -0.01 -0.04 0.07 -0.01 -0.04 0.05 -0.02 -0.01 0.01 ## PACF -0.01 0.00 -0.04 0.08 -0.05 0.02 -0.01 -0.02 0.03 0.00 ## [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198] ## ACF -0.05 -0.04 -0.01 0.01 0.04 -0.01 0.00 0.06 -0.06 -0.02 ## PACF -0.03 -0.04 0.01 -0.01 0.07 -0.01 0.02 -0.01 -0.03 0.01 ## [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208] ## ACF 0.02 0 0.00 0.00 -0.04 0.00 0.04 0.04 0.04 0.01 ## PACF 0.00 0 -0.02 -0.01 -0.05 -0.01 0.02 0.04 0.05 -0.01 # Assuming both P/ACF are tailing, fit a model sarima(oil_returns, p = 1, d = 0, q = 1) ## initial value -3.057594 ## iter 2 value -3.061420 ## iter 3 value -3.067360 ## iter 4 value -3.067479 ## iter 5 value -3.071834 ## iter 6 value -3.074359 ## iter 7 value -3.074843 ## iter 8 value -3.076656 ## iter 9 value -3.080467 ## iter 10 value -3.081546 ## iter 11 value -3.081603 ## iter 12 value -3.081615 ## iter 13 value -3.081642 ## iter 14 value -3.081643 ## iter 14 value -3.081643 ## iter 14 value -3.081643 ## final value -3.081643 ## converged ## initial value -3.082345 ## iter 2 value -3.082345 ## iter 3 value -3.082346 ## iter 4 value -3.082346 ## iter 5 value -3.082346 ## iter 5 value -3.082346 ## iter 5 value -3.082346 ## final value -3.082346 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 xmean ## -0.5264 0.7146 0.0018 ## s.e. 0.0871 0.0683 0.0022 ## ## sigma^2 estimated as 0.002102: log likelihood = 904.89, aic = -1801.79 ## ## $degrees_of_freedom ## [1] 541 ## ## $ttable ## Estimate SE t.value p.value ## ar1 -0.5264 0.0871 -6.0422 0.0000 ## ma1 0.7146 0.0683 10.4699 0.0000 ## xmean 0.0018 0.0022 0.7981 0.4252 ## ## $AIC ## [1] -3.312109 ## ## $AICc ## [1] -3.312027 ## ## $BIC ## [1] -3.280499 "],
["final-report.html", "11 Final Report 11.1 Basics of time series (when and why we use them, basic terminology) (1-2 pages) 11.2 Autoregressive models (1-2 pages) 11.3 Moving average models (1-2 pages) 11.4 Real-world Application using one of the above techniques (8 pages) 11.5 Forecasting. Key concepts and applying on dataset (2-3 pages) 11.6 Conclusion and looking into the future (1 page)", " 11 Final Report 13 pages of text 6 graphs/tables 11.1 Basics of time series (when and why we use them, basic terminology) (1-2 pages) 11.2 Autoregressive models (1-2 pages) 11.3 Moving average models (1-2 pages) 11.4 Real-world Application using one of the above techniques (8 pages) 11.4.1 An overview of dataset, why a time series model is appropriate (1-2 page) 11.4.2 Fitting AR and MR models to the data, explaining the parameters and why I choose them (4-5 pages) 11.4.3 Discussion &amp; Conclusion (2-3 pages) 11.5 Forecasting. Key concepts and applying on dataset (2-3 pages) 11.6 Conclusion and looking into the future (1 page) library(quantmod) getSymbols(&quot;CPIAUCSL&quot;, auto.assign = TRUE, src = &quot;FRED&quot;) ## [1] &quot;CPIAUCSL&quot; getSymbols(&quot;USSTHPI&quot;, auto.assign = TRUE, src = &quot;FRED&quot;) ## [1] &quot;USSTHPI&quot; CPI &lt;- read.csv(&quot;data/CPIAUCSL.csv&quot;) USHousePriceIndex &lt;- read.csv(&quot;data/USSTHPI.csv&quot;) plot(CPIAUCSL) plot(USSTHPI) "],
["references.html", "References", " References Time Series Fundamentals https://bookdown.org/gary_a_napier/time_series_lecture_notes/ChapterOne.html "]
]
