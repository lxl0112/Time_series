[
["index.html", "Time Series Preface", " Time Series Luke Le 2021-11-24 Preface A summary note of Time Series Fundamentals https://bookdown.org/gary_a_napier/time_series_lecture_notes/ChapterOne.html "],
["chapter-1-time-series-fundamentals.html", "1 Chapter 1: Time Series Fundamentals 1.1 Definition 1.2 White Noise: purely random process 1.3 Random Walk: random but dependent 1.4 Time Series Modelling 1.5 Time Series Properties 1.6 Stationarity", " 1 Chapter 1: Time Series Fundamentals 1.1 Definition \\[\\{X_t|t\\in T \\}\\] where \\(X_t\\) denotes random variables that are continuous, \\(T\\) denotes index sets that are discrete and equally spaced in time \\(x_t\\) denotes observations, or realisations of \\(X_t\\) 1.2 White Noise: purely random process \\[E[X_t]=\\mu\\] \\(Var[X_t]=\\sigma^2\\) where each \\(X_t\\) is independent It asssumes the observations are all independent 1.3 Random Walk: random but dependent \\[ X_T = X_{t-1} +Z_t\\] where \\(Z_t\\) is a purely random process with mean \\(\\mu\\) and variance \\(\\sigma^2\\) 1.4 Time Series Modelling Time series data are often decomposed into the following three components: Trend, Seasonal Effect, Unexplained variation Data with additive structure is easier to analyze as compared to multiplicative structure. If the time series data has multiplicative structure, we can model it by using transformations. 1.4.1 Log Transformation \\[log(X_t)=log(m_t\\cdot s_t\\cdot e_t)=log(m_t)+log(s_t)+log(e_t)\\] Use to stablize variance, make seasonal effect \\(s_t\\) additive, &amp; make the data normally distributed. 1.4.2 Box-Cox Transformation \\[ y_t=(x_t^\\lambda-1)/\\lambda \\space\\space \\lambda\\ne0\\] \\[ or \\space y_t=ln(x_t) \\space\\space \\lambda=0\\] where \\(\\lambda\\) is a tuning parameter chosen by the analyst 1.5 Time Series Properties 1.5.1 Mean function (if mean is constant) \\[\\mu_t=E[X_t]\\] \\[\\hat \\mu=\\frac 1n\\sum_{t=1}^{n}x_t\\] In case of walking average, see ch.2 1.5.2 Variance function (if variance is constant) \\[\\sigma_t^2=Var[X_t]=E[X_t^2]-E[X_t]^2\\] \\[\\hat \\sigma^2=\\frac1{n-1} \\sum_{t=1}^n{(x_t-\\hat\\mu)^2}\\] 1.5.3 Autocovariance function (ACVF): \\[\\gamma_{s,t}=Cov[X_s,X_t]=E[X_sX_t]-E[X_t]E[X_s]\\] where \\(\\gamma_{t,t}=Cov[X_t,X_t]=Var[X_t]=\\sigma_t^2\\) Real data: \\[\\gamma_\\tau=Cov[X_tX_{t+\\tau}]\\] with lag \\(\\tau=0,1,2,..\\) 1.5.4 Autocorrelation function (ACF): \\[\\rho_{s,t}=Corr[X_s,X_t]=\\frac{Cov[X_s,X_t]}{\\sqrt{Var[X_s]Var[X_t]}}=\\frac{\\gamma_{s,t}}{\\sigma_s\\sigma_t}\\] where \\(\\rho_{t,t}=Corr[X_t,X_t]=1\\) Real data: \\[\\rho_\\tau=Corr[X_tX_{t+\\tau}]=\\frac{Cov[X_t,X_{t+\\tau}]}{\\sqrt{Var[X_t]Var[X_{t+\\tau}]}}=\\frac{\\gamma_\\tau}{\\gamma_0}\\] 1.5.5 Properties Property 1: \\(\\rho_\\tau=\\rho_{-\\tau}\\) Property 2: \\(|\\rho_\\tau|\\leq1\\) Property 3: Invertibility is not assumed 1.6 Stationarity A time series process \\(\\{X_t|t\\in T\\}\\) is strictly stationary if the joint distribution \\(f(X_{t1},...,X_{tk})\\) is identical to the joint distribution \\(f(X_{t1+r},...,X_{tk+r})\\) for all collections \\(t_1,...,t_k\\) and separation values \\(r\\). In other words, shifting the time origin of the series by \\(r\\) has no effect on its joint distribution. A time series process \\(\\{X_t|t\\in T\\}\\) is weakly stationary (or second-order stationary) if 1, mean function is constant and finite; 2, variance function is constant and finite; 3, autocovariance and autocorrelation functions only depend on the lag. "],
["chapter-1-lab.html", "2 Chapter 1 Lab 2.1 White Noise (WN) 2.2 Random Walk (RW) 2.3 Stationary 2.4 ACF to Time Series Plot", " 2 Chapter 1 Lab Using R to dive in the building blocks of time series models. These building blocks include White Noise (WN), Random Walk (RW), Correlation analysis and ACF/PACF graphs, Autoregressive model (AR), and Moving Average (MA). First thing to note is that sampling frequency tend to be not exact. If the data was recorded daily or hourly, that is exact sampling frequency. But if data is recorded monthly or yearly, it is not since months differ in number of days. Second thing to know is detrending. Detrending is necessary to remove variability resulted from “trend” over time (oftentimes using diff() or fitting a linear regression model). Think of this as standardized for analysis. Sometimes the data needs log transformation before detrending. 2.1 White Noise (WN) Introduce the simulation function: library(gridExtra) WN_example_50 &lt;- arima.sim(model = list(order = c(0,0,0)), n = 50) WN_example_100 &lt;- arima.sim(model = list(order = c(0,0,0)), n = 100) WN_example_200 &lt;- arima.sim(model = list(order = c(0,0,0)), n = 200) WN_example_500 &lt;- arima.sim(model = list(order = c(0,0,0)), n = 500) ts.plot(WN_example_50) par(mfrow = c(3,1)) ts.plot(WN_example_100) ts.plot(WN_example_200) ts.plot(WN_example_500) # Plot your simulated data # plot.ts(cbind(WN_example_50, WN_example_100, WN_example_200, WN_example_500)) Above are white noise with various sample sizes. The c(0,0,0) parameter in arima() function specifies the WN model as it leaves only the independent error term \\(X_t=\\epsilon_t\\) Note that WN model has no pattern or trend. Named after white light in physics which display similar characteristics. 2.2 Random Walk (RW) \\[ X_T = X_{t-1} +Noise_t\\] with \\(Noise_t = Z_t (White noise)\\) Think of White Noise but with trend. The value of the next period depends on the value of the previous with a random level of noise. random_walk &lt;- arima.sim(model = list(order = c(0, 1, 0)), n = 100) plot(random_walk) Parameters 0,1,0 specifies Random Walk. Note that at \\(t=1\\), \\(X_{t-1}=0\\), meaning that the first data point in a random walk series is purely random error and the data points at higher time \\(t\\) is the previous value plus an independent error value of mean 0 and variance \\(\\hat \\sigma^2=1\\). Note that the plot would also be different every time it’s being regenerated due to its random nature. 2.3 Stationary Stationary is preferable since its behavior can be modeled with fewer parameter (which we will get into in ARIMA). 2.3.1 Examples: CPI Data library(quantmod) getSymbols(&quot;CPIAUCSL&quot;, auto.assign = TRUE, src = &quot;FRED&quot;) ## [1] &quot;CPIAUCSL&quot; # Here I imported the CPI data from the FRED database. CPI is nonstationary, its difference (which is inflation rate) is nonstationary, but inflation&#39;s difference is stationary plot(CPIAUCSL) CPI is nonstationary, its first order difference (which is inflation rate) is also nonstationary (mean not constant): plot(diff(CPIAUCSL)) Inflation’s difference (second order differencing) is stationary with variance increases over time plot(diff(diff(CPIAUCSL))) We will go in more detail on building a model and fitting it to the second order difference of CPI data in Chapter 6 Lab section. 2.4 ACF to Time Series Plot Plot 1 matches with B: Trend results in positive correlation in ealry lags with some cyclicity seen in later lags. Plot 2 matches with A: No trend results in both positive and negative correlation. Plot 3 matches with D: Obvious trend results in high positive correlation in early lags. Plot 3 is also seasonal, which can be seen in the pump of correlation at the cycle length of lag. Plot 4 matches with C: similar to 2 but correlation aren’t as strong Notes: Both upward and downward trend result in positive correlation in early lags (if there is more positive correlation in later lags, the trend is stronger) "],
["chapter-2-modelling-trends-and-seasonal-patterns.html", "3 Chapter 2: Modelling trends and seasonal patterns 3.1 Method 1: Regression 3.2 Method 2: Moving Average Smoothing 3.3 Method 3: Differencing 3.4 Choosing a smoothing parameter", " 3 Chapter 2: Modelling trends and seasonal patterns Trend and Seasonality can be the main interests Need to remove trend and seasonality to determine short-term correlation For this chapter, assume additivity: \\(X_t=m_t+s_t+e_t\\) (Refer to Ch.1) First is to estimate the trend and seasonal variation \\(\\hat m_t+\\hat s_t\\), then calculate the residual series \\(e_t^*=X_t-\\hat m_t-\\hat s_t\\) 3 of the most common methods for modelling trend and seasonality are described here. 3.1 Method 1: Regression \\[m_t+s_t=\\beta_0+\\beta_1z_{t1}+...+\\beta_pz_{tp}\\] Cons: OLS estimation assumes observations are independent. Pros: Remove trend to later model the correlation by a stationary time series process \\(z_{tn}\\) are often functions of time 3.1.1 Examples 3.1.1.1 Linear trend in time but no seasonal variation: Trend can be modelled by \\(X_t=\\beta_0+\\beta_1t+e_t\\) and using series \\(e_t\\) to analyze short-term correlation 3.1.1.2 Quadratic trend in time but no seasonal variation: Trend can be modeled by \\(X_t=\\beta_0+\\beta_1t+\\beta_2t^2+e_t\\) 3.1.1.3 Seasonal pattern in time and a linear trend with 365 data points: Trend and Seasonality can be modeled by \\(X_t=\\beta_0+\\beta_1t+\\beta_2sin(8\\pi t/365)+e_t\\) 3.1.2 Other common models 3.1.2.1 Other Covariates \\[m_t=\\beta+0+\\beta_1\\alpha_t\\] 3.1.2.2 Polynomials \\[m_t=\\beta+0+\\beta_1t+\\beta_qt^q\\] Higher \\(q\\), more flexible the trend 3.1.2.3 Harmonics \\[s_t=\\beta_0+\\sum_{i=1}^q\\beta_{1i}sin(w_it)+\\beta_{2i}cos(w_it)\\] Harmonics assume the seasonal pattern has a regular shape, i.e. the height of the peaks is the same as the depth of the troughs. 3.1.2.4 Seasonal Factors Assuming the seasonal pattern repeats itself every d time points, a less restrictive approach is to model it as: \\[s_{t}=\\left\\{\\begin{array}{cc}0&amp;\\mbox{if}~t=1, d+1, 2d+2,\\ldots\\\\ s_{2}&amp;\\mbox{if}~t=2, d+2, 2d+2,\\ldots\\\\ \\vdots&amp;\\vdots\\\\ s_{d}&amp;\\mbox{if}~t=d, 2d, 3d,\\ldots\\\\\\end{array}\\right.\\] This model can be fitted by creating \\(d - 1\\) dummy variables in the design matrix, that contain 1’s and 0’s. 3.1.2.5 Natural cubic splines (More flexible) \\[m_{t}+s_{t}=\\beta_{0}+\\sum_{i=1}^{q}\\beta_{i}B_{i}(t)\\] 3.2 Method 2: Moving Average Smoothing A Moving average smoother estimates the trend and seasonal variation at time t by averaging the current observation and the q either side \\[\\hat{m}_{t}+\\hat{s}_{t}=\\frac{1}{2q+1}\\sum_{j=-q}^{q}x_{t-j}\\] Note: Shorten length of time series by \\(2q\\), therefore if the trend is smooth and \\(q\\) is large, the series shortens significantly. 3.3 Method 3: Differencing 3.3.1 Remove Trends 3.3.1.1 First order difference operator \\(\\nabla\\): \\[\\nabla X_{t}=X_{t}-X_{t-1}=(1-B)X_{t}\\] where \\(B\\) is the Backshift operator defined as \\(BX_{t}=X_{t-1}\\) 3.3.1.2 General order difference operator \\(\\nabla^q\\) \\[\\nabla^{q} X_{t}=\\nabla[\\nabla^{q-1}X_{t}]\\] \\[B^{q}X_{t}=X_{t-q}\\] Notes: 1. A polynomial trend of order \\(q\\) can be removed by \\(q^{th}\\) order differencing 2. Typically only first or second order differencing is required 3. Shortens length by \\(q\\) 4. Differencing won’t allow to estimate the trend but only to remove it. 3.3.2 Remove Seasonality The seasonal difference of order \\(d\\) is the operator \\(\\nabla_d\\) given by \\[\\nabla_{d} X_{t}=X_{t}-X_{t-d}=(1-B^{d})X_{t}\\] Notes: 1. Trial and Error approach 2. No point differencing twice if once is adequate 3. Differencing increases the variance 3.4 Choosing a smoothing parameter 3.4.1 Simplicity Simple statistical models are preferred since they are easier to make inferences from. 3.4.2 Objective criteria (AIC &amp; BIC) Assumptions: 1. Normally distributed observations 2. \\(m_{t}+s_{t}=\\textbf{z}_{t}^\\top \\boldsymbol{\\beta}\\) where \\(\\textbf{z}_{t}^\\top\\) is a vector of \\(q-1\\) known covariates and a one to represent the intercept term 3.4.2.1 Akaike’s Information Criterion (AIC) \\[\\mbox{AIC}(q)=-2\\mathcal{L}(\\mathbf{x}|\\hat{\\boldsymbol{\\beta}}) + 2q\\] 3.4.2.2 Bayesian Information Criterion (BIC) \\[\\mbox{BIC}(q)=-2\\mathcal{L}(\\mathbf{x}|\\hat{\\boldsymbol{\\beta}}) + \\ln(n)q\\] where \\(\\mathcal{L}(\\mathbf{x}|\\hat{\\boldsymbol{\\beta}})\\) is the maximised log likelihood function of \\(\\mathbf{x}\\) Both criteria tradeoff the fit to the data against simplicity (i.e. few parameters), and small values suggest a good fit to the data. "],
["chapter-2-lab.html", "4 Chapter 2 Lab 4.1 Model choice and Residual analysis 4.2 Data Example: Annual Varve Series", " 4 Chapter 2 Lab 4.1 Model choice and Residual analysis AIC and BIC are great measures for fit of models: \\[ average(observed-predicted)^2+k(p+q)\\] The smaller these measures are, the better the prediction. sarima() fucntion returns 4 graphs. Given we want the model to predict most of the observed data, and the residuals are just white noise, these 4 tools can help us identify whether residuals are just white noise: Standardized residuals: does it look like white noise? Sample ACF of residuals: no statistically significant ACF (everything within the blue lines) Normal Q-Q plot: it should look like a line with a few outliers Q-statistic p-values: all p-values should be above the line Finding the right model most of the time are through trials and errors starting with some ideas based off what we know about the data. 4.2 Data Example: Annual Varve Series Sedimentary deposits from one location in Massachusetts for 634 years, beginning nearly 12,000 years ago. Loaded from library astsa library(astsa) ts.plot(varve) dl_varve &lt;- diff(log(varve)) ts.plot(dl_varve) From the time series plot, it does not look like the data has a obvious linear trend, therefore first order differencing is most appropriate to tranform the data into a stationary process. It looks like it has a constant mean but moving average, suggesting MA(q) models might be appropriate. We will take a look at the ACF and PACF plots to further access: acf2(varve) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## ACF 0.59 0.52 0.44 0.42 0.39 0.40 0.36 0.34 0.32 0.29 0.30 0.30 0.27 0.33 ## PACF 0.59 0.25 0.11 0.11 0.07 0.11 0.03 0.02 0.02 0.01 0.06 0.04 -0.01 0.14 ## [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] ## ACF 0.32 0.28 0.27 0.26 0.24 0.28 0.31 0.30 0.31 0.28 0.27 0.25 ## PACF 0.05 -0.04 0.00 0.00 0.00 0.08 0.08 0.03 0.04 0.00 0.00 -0.04 ## [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## ACF 0.26 0.24 0.23 0.20 0.2 0.24 0.21 0.22 0.23 0.19 ## PACF 0.03 -0.02 -0.02 -0.02 0.0 0.09 0.00 0.02 0.02 -0.07 The correlograms on the time plot of varve did not tell us a great deal here, which is expected. Now we will look at the ACF and PACF of the first order difference series: acf2(dl_varve) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## ACF -0.4 -0.04 -0.06 0.01 0.00 0.04 -0.04 0.04 0.01 -0.05 0.06 -0.06 ## PACF -0.4 -0.24 -0.23 -0.18 -0.15 -0.08 -0.11 -0.05 -0.01 -0.07 0.02 -0.05 ## [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] ## ACF -0.04 0.08 -0.02 0.01 0.00 0.03 -0.05 -0.06 0.07 0.04 -0.06 0.05 ## PACF -0.12 -0.03 -0.05 -0.04 -0.03 0.03 -0.03 -0.13 -0.04 0.01 -0.06 0.01 ## [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## ACF -0.01 -0.04 0.05 -0.05 0.03 -0.02 0.00 0.06 -0.05 -0.03 0.04 -0.05 ## PACF 0.02 -0.04 0.03 -0.02 0.00 -0.03 -0.02 0.04 -0.02 -0.02 0.01 -0.07 Based on ACF, MA(1) model might be appropriate. Based on PACF, AR(5) might be appropriate, however, I would start with smallest value of p for simplicity of the model # Fit an MA(1) to dl_varve. sarima(dl_varve, p = 0, d = 0, q = 1) ## initial value -0.551780 ## iter 2 value -0.671633 ## iter 3 value -0.706234 ## iter 4 value -0.707586 ## iter 5 value -0.718543 ## iter 6 value -0.719692 ## iter 7 value -0.721967 ## iter 8 value -0.722970 ## iter 9 value -0.723231 ## iter 10 value -0.723247 ## iter 11 value -0.723248 ## iter 12 value -0.723248 ## iter 12 value -0.723248 ## iter 12 value -0.723248 ## final value -0.723248 ## converged ## initial value -0.722762 ## iter 2 value -0.722764 ## iter 3 value -0.722764 ## iter 4 value -0.722765 ## iter 4 value -0.722765 ## iter 4 value -0.722765 ## final value -0.722765 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 xmean ## -0.7710 -0.0013 ## s.e. 0.0341 0.0044 ## ## sigma^2 estimated as 0.2353: log likelihood = -440.68, aic = 887.36 ## ## $degrees_of_freedom ## [1] 631 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.7710 0.0341 -22.6002 0.0000 ## xmean -0.0013 0.0044 -0.2818 0.7782 ## ## $AIC ## [1] 1.401826 ## ## $AICc ## [1] 1.401856 ## ## $BIC ## [1] 1.422918 model_ma1 &lt;- arima(dl_varve, order = c(0,0,1)) AIC(model_ma1) ## [1] 887.3557 BIC(model_ma1) ## [1] 900.7071 # Fit an MA(2) to dl_varve. Improvement? sarima(dl_varve, p = 0, d = 0, q = 2) ## initial value -0.551780 ## iter 2 value -0.679736 ## iter 3 value -0.728605 ## iter 4 value -0.734640 ## iter 5 value -0.735449 ## iter 6 value -0.735979 ## iter 7 value -0.736015 ## iter 8 value -0.736059 ## iter 9 value -0.736060 ## iter 10 value -0.736060 ## iter 11 value -0.736061 ## iter 12 value -0.736061 ## iter 12 value -0.736061 ## iter 12 value -0.736061 ## final value -0.736061 ## converged ## initial value -0.735372 ## iter 2 value -0.735378 ## iter 3 value -0.735379 ## iter 4 value -0.735379 ## iter 4 value -0.735379 ## iter 4 value -0.735379 ## final value -0.735379 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 ma2 xmean ## -0.6710 -0.1595 -0.0013 ## s.e. 0.0375 0.0392 0.0033 ## ## sigma^2 estimated as 0.2294: log likelihood = -432.69, aic = 873.39 ## ## $degrees_of_freedom ## [1] 630 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.6710 0.0375 -17.9057 0.0000 ## ma2 -0.1595 0.0392 -4.0667 0.0001 ## xmean -0.0013 0.0033 -0.4007 0.6888 ## ## $AIC ## [1] 1.379757 ## ## $AICc ## [1] 1.379817 ## ## $BIC ## [1] 1.40788 model_ma2 &lt;- arima(dl_varve, order = c(0,0,2)) AIC(model_ma2) ## [1] 873.3861 BIC(model_ma2) ## [1] 891.188 # Fit an ARMA(1,1) to dl_varve. Improvement? sarima(dl_varve, p = 1, d = 0, q = 1) ## initial value -0.550994 ## iter 2 value -0.648962 ## iter 3 value -0.676965 ## iter 4 value -0.699167 ## iter 5 value -0.724554 ## iter 6 value -0.726719 ## iter 7 value -0.729066 ## iter 8 value -0.731976 ## iter 9 value -0.734235 ## iter 10 value -0.735969 ## iter 11 value -0.736410 ## iter 12 value -0.737045 ## iter 13 value -0.737600 ## iter 14 value -0.737641 ## iter 15 value -0.737643 ## iter 16 value -0.737643 ## iter 17 value -0.737643 ## iter 18 value -0.737643 ## iter 18 value -0.737643 ## iter 18 value -0.737643 ## final value -0.737643 ## converged ## initial value -0.737522 ## iter 2 value -0.737527 ## iter 3 value -0.737528 ## iter 4 value -0.737529 ## iter 5 value -0.737530 ## iter 5 value -0.737530 ## iter 5 value -0.737530 ## final value -0.737530 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 xmean ## 0.2341 -0.8871 -0.0013 ## s.e. 0.0518 0.0292 0.0028 ## ## sigma^2 estimated as 0.2284: log likelihood = -431.33, aic = 870.66 ## ## $degrees_of_freedom ## [1] 630 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2341 0.0518 4.5184 0.0000 ## ma1 -0.8871 0.0292 -30.4107 0.0000 ## xmean -0.0013 0.0028 -0.4618 0.6444 ## ## $AIC ## [1] 1.375456 ## ## $AICc ## [1] 1.375517 ## ## $BIC ## [1] 1.403579 model_arma11 &lt;- arima(dl_varve, order = c(1,0,1)) AIC(model_arma11) ## [1] 870.6638 BIC(model_arma11) ## [1] 888.4657 # Fit an ARMA(2,1) to dl_varve. Improvement? sarima(dl_varve, p = 2, d = 0, q = 1) ## initial value -0.550651 ## iter 2 value -0.656533 ## iter 3 value -0.691671 ## iter 4 value -0.699615 ## iter 5 value -0.714090 ## iter 6 value -0.721637 ## iter 7 value -0.724629 ## iter 8 value -0.731357 ## iter 9 value -0.733142 ## iter 10 value -0.733786 ## iter 11 value -0.734198 ## iter 12 value -0.734623 ## iter 13 value -0.735038 ## iter 14 value -0.735325 ## iter 15 value -0.735348 ## iter 16 value -0.735350 ## iter 17 value -0.735350 ## iter 17 value -0.735350 ## iter 17 value -0.735350 ## final value -0.735350 ## converged ## initial value -0.737984 ## iter 2 value -0.738073 ## iter 3 value -0.738128 ## iter 4 value -0.738306 ## iter 5 value -0.738336 ## iter 6 value -0.738337 ## iter 6 value -0.738337 ## iter 6 value -0.738337 ## final value -0.738337 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ar2 ma1 xmean ## 0.2447 0.0483 -0.9055 -0.0013 ## s.e. 0.0501 0.0473 0.0298 0.0026 ## ## sigma^2 estimated as 0.228: log likelihood = -430.82, aic = 871.64 ## ## $degrees_of_freedom ## [1] 629 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.2447 0.0501 4.8837 0.0000 ## ar2 0.0483 0.0473 1.0192 0.3085 ## ma1 -0.9055 0.0298 -30.3958 0.0000 ## xmean -0.0013 0.0026 -0.5044 0.6141 ## ## $AIC ## [1] 1.377002 ## ## $AICc ## [1] 1.377102 ## ## $BIC ## [1] 1.412156 model_arma21 &lt;- arima(dl_varve, order = c(2,0,1)) AIC(model_arma21) ## [1] 871.6421 BIC(model_arma21) ## [1] 893.8945 Here we tried fitting ARIMA(0,0,1), ARIMA(0,0,2), ARIMA(1,0,1), and ARIMA(2,0,1), which are MA(1), MA(2), ARMA(1,1), and ARMA(2,1), respectively. ARMA(1,1) and ARMA(2,1) are better options than MA(1) and MA(2) based on the AIC and BIC and other statistics tell that the residuals resemble more of white noise. "],
["chapter-3-autoregressive-processes.html", "5 Chapter 3: Autoregressive processes 5.1 Definition 5.2 First order Autoregressive process 5.3 \\(AR(p)\\) process 5.4 When and how to use AR(p) model", " 5 Chapter 3: Autoregressive processes \\(Z_t\\) is purely random with mean zero and variance \\(\\sigma_z^2\\) 5.1 Definition An Autoregressive process of order \\(p\\), denoted \\(AR(p)\\), is given by \\[X_{t}=\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t}\\] Where we assume \\(X_{0}=X_{-1}=\\ldots=X_{1-p}=0\\) Essentially regress \\(X_t\\) to its own past values 5.2 First order Autoregressive process An \\(AR(1)\\) process is given by \\[X_{t}=\\alpha X_{t-1}+Z_{t}\\] 5.2.1 Mean \\[E(AR(1))=0\\] 5.2.2 Variance If \\(|\\alpha|\\geq1\\) then \\(Var[X_t]=\\infty\\) If \\(|\\alpha|\\lt1\\) then \\(\\alpha^2\\lt1\\), and: \\[\\sum_{j=0}^{\\infty}\\alpha^{2j}=1+\\alpha^{2}+\\alpha^{4}+\\alpha^{6}+\\ldots~=~\\frac{1}{1-\\alpha^{2}}\\] So we have: \\[\\mathrm{Var}[X_{t}]=\\sigma_{z}^{2}\\sum_{j=0}^{\\infty}\\alpha^{2j}=\\frac{\\sigma_{z}^{2}}{1-\\alpha^{2}} \\] 5.2.3 Examples 5.3 \\(AR(p)\\) process is given by \\[X_{t}=\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t} \\] \\(Z_t\\) can be written as: \\[\\begin{eqnarray} X_{t}&amp;=&amp;\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t}\\nonumber\\\\ X_{t}-\\alpha_{1}X_{t-1}-\\ldots-\\alpha_{p}X_{t-p}&amp;=&amp;Z_{t}\\nonumber\\\\ (1-\\alpha_{1}B-\\alpha_{2}B^{2}-\\ldots-\\alpha_{p}B^{p})X_{t}&amp;=&amp;Z_{t}\\nonumber\\\\ \\phi(B)X_{t}&amp;=&amp;Z_{t}\\nonumber \\end{eqnarray} \\] where \\(B\\) is the backshift operator 5.3.1 Mean \\[E[AP(p)] = 0\\] 5.3.2 Stationarity We calculate the variance and autocorrelation function conditional on the process being stationary. The following theorem tells us when an AR(p) process is stationary. Theorem AR(p) written as \\(\\phi(B)X_{t}=Z_{t}\\) where \\(\\phi(B)\\) is the characteristic polynomial \\(\\phi(B)=1-\\alpha_{1}B-\\alpha_{2}B^{2}-\\ldots-\\alpha_{p}B^{p}\\) Then the process is stationary if the roots of the characteristic equation \\[\\phi(B)=1-\\alpha_{1}B-\\alpha_{2}B^{2}-\\ldots-\\alpha_{p}B^{p}=0\\] have modulus greater than 1, i.e. they lie outside the unit circle. Here we consider \\(B\\) as the variable of the polynominal equation 5.3.3 Variance \\[\\mathrm{Var}[X_{t}]=\\sigma^{2}_{z}+\\sum_{\\tau=1}^{p}\\alpha_{\\tau}\\gamma_{\\tau} \\] 5.3.4 Autocorrelation Function Using Yule-Walker equation: \\[\\rho_{\\tau}=\\alpha_{1}\\rho_{\\tau-1}+\\ldots+\\alpha_{p}\\rho_{\\tau-p}\\] Examples for \\(\\tau = 1\\) and \\(\\tau = 2\\): \\[\\rho_{1}=\\frac{\\alpha_{1}}{1-\\alpha_{2}}.\\] \\[\\rho_{2}=\\alpha_{1}\\rho_{1}+\\alpha_{2}\\rho_{0}=\\frac{\\alpha_{1}^{2}}{1-\\alpha_{2}}+\\alpha_{2}\\] Note: as \\(\\tau\\) increases the function gets more complex algebraically. Same thing with p increases in AR(p) 5.4 When and how to use AR(p) model Realisations from the following AR(1) and AR(2) models: \\[X_t=0.6X_{t-1}+Z_t\\] \\[X_t=0.75X_{t-1}-0.125X_{t-2}+Z_t\\] # Simulate AR(1) and AR(2) data n &lt;- 100 sd &lt;- 10 data1 &lt;- arima.sim(model = list(ar = 0.6), n = n, sd = sd) data2 &lt;- arima.sim(model = list(ar = c(0.75, -0.125)), n = n, sd = sd) # Plot the correlogram and time plot par(mfrow = c(2, 2)) plot(data1, main = &quot;AR(1) data&quot;) acf(data1, main = &quot;AR(1) data&quot;) plot(data2, main = &quot;AR(2) data&quot;) acf(data2, main = &quot;AR(2) data&quot;) The ACF are the same. It will be better to look at the correlogram of the residuals to see if the residuals resemble a purely random process. If there is still residual correlation, increase \\(p\\) by one and re-fit the model and repeat. However, this is not a good method if AR(p) isn’t a suitable model at the first place. 5.4.1 Definition of PACF The partial autocorrelation function (PACF) at lag \\(τ\\) is equal to the estimated lag \\(τ\\) coefficient \\(\\hat \\alpha_{\\tau}\\), obtained when fitting an AR(\\(τ\\)) model to a data set. It is denoted by \\(π_τ\\), and represents the excess correlation in the time series that has not been accounted for by the \\(τ-1\\) smaller lags. The partial autocorrelation function allows us to determine an appropriate AR(p) process for a given data set. This is because if the data do come from an AR(p) process then: \\[\\pi_{\\tau} = \\left\\{\\begin{array}{cc}\\mbox{positive}&amp;\\tau\\leq p\\\\0&amp;\\tau&gt;p\\end{array}\\right.\\] This is because by definition, PACF is only measuring the direct correlation of the data with lag \\(\\tau\\). The AR(p) model does not factor in lag bigger than p, making the coefficient of those would be 0. Therefore, to choose the order of an AR(p) process, plot the PACF and choose p as the smallest value that is significantly different from zero. 5.4.1.1 Example Looking at PACF, Data 1 uses AR(1) process, Data 2 uses AR(2), Data 3 uses AR(3), Data 4 uses AR(12) "],
["chapter-3-lab.html", "6 Chapter 3 Lab 6.1 Autoregressive model (AR) 6.2 Chicken Price 6.3 Chicken price 6.4 Unemployment", " 6 Chapter 3 Lab 6.1 Autoregressive model (AR) AR(p) process is correlation with lagged values of the data itself. \\[ X_t-\\mu = \\alpha \\cdot (X_{t-1}-\\mu)+Noise_t\\] Some simulation examples down below. Note how the coefficent \\(\\phi\\) affects the look of the graph: x &lt;- arima.sim(model = list(ar = 0.5), n = 100) # Simulate an AR model with 0.9 slope y &lt;- arima.sim(model = list(ar = 0.9), n = 100) # Simulate an AR model with -0.75 slope z &lt;- arima.sim(model = list(ar = -0.75), n = 100) plot.ts(cbind(x, y, z)) AR(1) model with slope \\(\\alpha=0.9\\) creates a obvious trend while the negative slope creates a zic-zac pattern. par(mfrow = c(3,1)) # Plot your simulated data acf(x) acf(y) acf(z) The ACF plots tell a lot about each time plot: Strong short-term positive correlation in early lags (1 and 2) Strong short-term positive correlation in early lags up to lag of 5 Strong short-term negative correlation in early lags up to lag of 5 The PACF will tell us what AR(p) model would be appropriate for fitting this data par(mfrow = c(3,1)) # Plot your simulated data pacf(x) pacf(y) pacf(z) The PACF plots tell us that a AR(1) model would be appropriate for all 3 data. This is expected since we used AR(1) models to generate these data. 6.2 Chicken Price 6.3 Chicken price library(astsa) # Plot differenced chicken plot(diff(chicken)) # Plot P/ACF pair of differenced data to lag 60 acf2(diff(chicken), max.lag = 60) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## ACF 0.72 0.39 0.09 -0.07 -0.16 -0.20 -0.27 -0.23 -0.11 0.09 0.26 0.33 ## PACF 0.72 -0.29 -0.14 0.03 -0.10 -0.06 -0.19 0.12 0.10 0.16 0.09 0.00 ## [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] ## ACF 0.20 0.07 -0.03 -0.10 -0.19 -0.25 -0.29 -0.20 -0.08 0.08 0.16 0.18 ## PACF -0.22 0.03 0.03 -0.11 -0.09 0.01 -0.03 0.07 -0.04 0.06 -0.05 0.02 ## [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## ACF 0.08 -0.06 -0.21 -0.31 -0.40 -0.40 -0.33 -0.18 0.02 0.20 0.30 0.35 ## PACF -0.14 -0.19 -0.13 -0.06 -0.08 -0.05 0.01 0.03 0.10 0.02 -0.01 0.09 ## [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] ## ACF 0.26 0.13 -0.02 -0.14 -0.23 -0.21 -0.18 -0.11 -0.03 0.08 0.21 0.33 ## PACF -0.12 0.01 -0.01 -0.05 0.02 0.12 -0.05 -0.13 -0.07 0.01 0.14 0.05 ## [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] ## ACF 0.26 0.12 -0.01 -0.11 -0.13 -0.09 -0.09 -0.06 0.03 0.17 0.29 0.32 ## PACF -0.20 -0.01 0.07 -0.04 0.02 0.00 -0.08 0.03 0.04 0.00 0.01 0.03 # Fit ARIMA(2,1,0) to chicken - not so good sarima(chicken, p = 2, d = 1, q = 0) ## initial value 0.001863 ## iter 2 value -0.156034 ## iter 3 value -0.359181 ## iter 4 value -0.424164 ## iter 5 value -0.430212 ## iter 6 value -0.432744 ## iter 7 value -0.432747 ## iter 8 value -0.432749 ## iter 9 value -0.432749 ## iter 10 value -0.432751 ## iter 11 value -0.432752 ## iter 12 value -0.432752 ## iter 13 value -0.432752 ## iter 13 value -0.432752 ## iter 13 value -0.432752 ## final value -0.432752 ## converged ## initial value -0.420883 ## iter 2 value -0.420934 ## iter 3 value -0.420936 ## iter 4 value -0.420937 ## iter 5 value -0.420937 ## iter 6 value -0.420937 ## iter 6 value -0.420937 ## iter 6 value -0.420937 ## final value -0.420937 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ar2 constant ## 0.9494 -0.3069 0.2632 ## s.e. 0.0717 0.0718 0.1362 ## ## sigma^2 estimated as 0.4286: log likelihood = -178.64, aic = 365.28 ## ## $degrees_of_freedom ## [1] 176 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.9494 0.0717 13.2339 0.0000 ## ar2 -0.3069 0.0718 -4.2723 0.0000 ## constant 0.2632 0.1362 1.9328 0.0549 ## ## $AIC ## [1] 2.040695 ## ## $AICc ## [1] 2.041461 ## ## $BIC ## [1] 2.111922 # Fit SARIMA(2,1,0,1,0,0,12) to chicken - that works sarima(chicken, p = 2, d = 1, q = 0, P = 1, D = 0, Q = 0, S = 12) ## initial value 0.015039 ## iter 2 value -0.226398 ## iter 3 value -0.412955 ## iter 4 value -0.460882 ## iter 5 value -0.470787 ## iter 6 value -0.471082 ## iter 7 value -0.471088 ## iter 8 value -0.471090 ## iter 9 value -0.471092 ## iter 10 value -0.471095 ## iter 11 value -0.471095 ## iter 12 value -0.471096 ## iter 13 value -0.471096 ## iter 14 value -0.471096 ## iter 15 value -0.471097 ## iter 16 value -0.471097 ## iter 16 value -0.471097 ## iter 16 value -0.471097 ## final value -0.471097 ## converged ## initial value -0.473585 ## iter 2 value -0.473664 ## iter 3 value -0.473721 ## iter 4 value -0.473823 ## iter 5 value -0.473871 ## iter 6 value -0.473885 ## iter 7 value -0.473886 ## iter 8 value -0.473886 ## iter 8 value -0.473886 ## iter 8 value -0.473886 ## final value -0.473886 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ar2 sar1 constant ## 0.9154 -0.2494 0.3237 0.2353 ## s.e. 0.0733 0.0739 0.0715 0.1973 ## ## sigma^2 estimated as 0.3828: log likelihood = -169.16, aic = 348.33 ## ## $degrees_of_freedom ## [1] 175 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.9154 0.0733 12.4955 0.0000 ## ar2 -0.2494 0.0739 -3.3728 0.0009 ## sar1 0.3237 0.0715 4.5238 0.0000 ## constant 0.2353 0.1973 1.1923 0.2347 ## ## $AIC ## [1] 1.945971 ## ## $AICc ## [1] 1.947256 ## ## $BIC ## [1] 2.035005 6.4 Unemployment plot(unemp) d_unemp &lt;- diff(unemp) plot(d_unemp) # Plot seasonal differenced diff_unemp dd_unemp &lt;- diff(d_unemp, lag = 12) plot(dd_unemp) # Plot P/ACF pair of the fully differenced data to lag 60 dd_unemp &lt;- diff(diff(unemp), lag = 12) acf2(dd_unemp, max.lag = 60) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## ACF 0.21 0.33 0.15 0.17 0.10 0.06 -0.06 -0.02 -0.09 -0.17 -0.08 -0.48 -0.18 ## PACF 0.21 0.29 0.05 0.05 0.01 -0.02 -0.12 -0.03 -0.05 -0.15 0.02 -0.43 -0.02 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## ACF -0.16 -0.11 -0.15 -0.09 -0.09 0.03 -0.01 0.02 -0.02 0.01 -0.02 0.09 ## PACF 0.15 0.03 -0.04 -0.01 0.00 0.01 0.01 -0.01 -0.16 0.01 -0.27 0.05 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## ACF -0.05 -0.01 0.03 0.08 0.01 0.03 -0.05 0.01 0.02 -0.06 -0.02 -0.12 ## PACF -0.01 -0.05 0.05 0.09 -0.04 0.02 -0.07 -0.01 -0.08 -0.08 -0.23 -0.08 ## [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## ACF 0.01 -0.03 -0.03 -0.10 -0.02 -0.13 0.00 -0.06 0.01 0.02 0.11 0.13 ## PACF 0.06 -0.07 -0.01 0.03 -0.03 -0.11 -0.04 0.01 0.00 -0.03 -0.04 0.02 ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] ## ACF 0.10 0.07 0.10 0.12 0.06 0.14 0.05 0.04 0.04 0.07 -0.03 ## PACF 0.03 -0.05 0.02 0.02 -0.08 0.00 -0.03 -0.07 0.05 0.04 -0.04 # Fit an appropriate model sarima(unemp, p = 2, d = 1, q = 0, P = 0, D = 1, Q = 1, S = 12) ## initial value 3.340809 ## iter 2 value 3.105512 ## iter 3 value 3.086631 ## iter 4 value 3.079778 ## iter 5 value 3.069447 ## iter 6 value 3.067659 ## iter 7 value 3.067426 ## iter 8 value 3.067418 ## iter 8 value 3.067418 ## final value 3.067418 ## converged ## initial value 3.065481 ## iter 2 value 3.065478 ## iter 3 value 3.065477 ## iter 3 value 3.065477 ## iter 3 value 3.065477 ## final value 3.065477 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), include.mean = !no.constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ar2 sma1 ## 0.1351 0.2464 -0.6953 ## s.e. 0.0513 0.0515 0.0381 ## ## sigma^2 estimated as 449.6: log likelihood = -1609.91, aic = 3227.81 ## ## $degrees_of_freedom ## [1] 356 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.1351 0.0513 2.6326 0.0088 ## ar2 0.2464 0.0515 4.7795 0.0000 ## sma1 -0.6953 0.0381 -18.2362 0.0000 ## ## $AIC ## [1] 8.723811 ## ## $AICc ## [1] 8.723988 ## ## $BIC ## [1] 8.765793 "],
["chapter-4-moving-average-processes.html", "7 Chapter 4: Moving Average processes 7.1 Definition 7.2 Mean and Variance 7.3 Autocorrelation functions 7.4 Invertibility 7.5 MA Model Identification 7.6 MA parameter estimation 7.7 MA Example", " 7 Chapter 4: Moving Average processes 7.1 Definition A Moving Average process of order \\(q\\), denoted MA(\\(q\\)), is given by \\[X_{t}=\\lambda_{0}Z_{t}+\\lambda_{1}Z_{t-1}+\\ldots+\\lambda_{q}Z_{t-q}\\] where \\(\\lambda_0=1\\) 7.2 Mean and Variance Mean of any MA(\\(q\\)) process: \\(E[X_t]=0\\) Variance: \\[X_{t}=\\sigma^{2}_{z}\\left[1+\\sum_{j=1}^{q}\\lambda_{j}^{2}\\right]\\] 7.3 Autocorrelation functions Autocovariance function: \\[\\gamma_{\\tau}=\\mathrm{Cov}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}\\sigma_{z}^{2}\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}&amp;\\mbox{if}~\\tau=0,1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right.\\] and autocorrelation function: \\[\\rho_{\\tau}=\\mathrm{Corr}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}}{\\sum_{j=0}^{q}\\lambda_{j}^{2}}&amp;\\mbox{if}~\\tau=1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right.\\] where \\(\\lambda_0=1\\) Notes: The mean and variance of any MA(\\(q\\)) process are finite and constant, while the autocorrelation function is finite and does not depend on \\(t\\). Therefore any MA(\\(q\\)) is weakly stationary. The autocorrelation function of an MA(\\(q\\)) process is positive at lags \\(1,…,q\\) and zero for any lag greater than \\(q\\). This gives us a method for detecting whether an MA(\\(q\\)) process is an appropriate model for a given data set. 7.3.1 Examples: Consider the MA(1) process: \\(X_t = Z_t + \\lambda Z_{t-1}\\). Its variance is given by \\[\\begin{eqnarray} \\mathrm{Var}[X_{t}]&amp;=&amp;\\mathrm{Var}[Z_{t}+\\lambda Z_{t-1}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}(1+\\lambda^{2}).\\nonumber \\end{eqnarray}\\] Its lag one autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+1}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+\\lambda Z_{t-1},~Z_{t+1}+\\lambda Z_{t}]\\nonumber\\\\ &amp;=&amp;\\mathrm{Cov}[Z_{t},~Z_{t+1}]+\\lambda \\mathrm{Cov}[Z_{t},~Z_{t}]+\\lambda \\mathrm{Cov}[Z_{t-1},~Z_{t+1}]+\\lambda^{2}\\mathrm{Cov}[Z_{t-1},~Z_{t}]\\nonumber\\\\ &amp;=&amp;\\lambda \\sigma_{z}^{2}\\nonumber \\end{eqnarray}\\] lag 2 autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+2}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+\\lambda Z_{t-1},~Z_{t+2}+\\lambda Z_{t+1}]\\nonumber\\\\ &amp;=&amp;\\mathrm{Cov}[Z_{t},~Z_{t+2}]+\\lambda \\mathrm{Cov}[Z_{t},~Z_{t+1}]+\\lambda \\mathrm{Cov}[Z_{t-1},~Z_{t+2}]+\\lambda^{2}\\mathrm{Cov}[Z_{t-1},~Z_{t+1}]\\nonumber\\\\ &amp;=&amp;0.\\nonumber \\end{eqnarray}\\] the autocorrelation function is given by \\[\\rho_{\\tau}=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\lambda}{1+\\lambda^{2}}&amp;\\mbox{if}~\\tau=1\\\\ 0&amp;\\tau&gt;1\\\\ \\end{array}\\right.\\] 7.3.2 Another example Consider the MA(2) process \\(X_t = Z_t +0.9Z_{t-1}+0.5Z_{t-2}\\). Its variance is given by \\[\\begin{eqnarray} \\mathrm{Var}[X_{t}]&amp;=&amp;\\mathrm{Var}[Z_{t}+0.9 Z_{t-1} + 0.5Z_{t-2}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}(1+0.81 + 0.25)\\nonumber\\\\ &amp;=&amp;2.06\\sigma^{2}_{z}.\\nonumber \\end{eqnarray} \\] Its lag one autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+1}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+0.9 Z_{t-1}+0.5Z_{t-2},~Z_{t+1}+0.9 Z_{t} +0.5Z_{t-1}]\\nonumber\\\\ &amp;=&amp;0.9 \\mathrm{Cov}[Z_{t},~Z_{t}]+0.45 \\mathrm{Cov}[Z_{t-1},~Z_{t-1}]\\nonumber\\\\ &amp;=&amp;1.35\\sigma_{z}^{2}\\nonumber \\end{eqnarray} \\] Its lag 2 autocovariance is given by \\[\\begin{eqnarray} \\mathrm{Cov}[X_{t},X_{t+2}]&amp;=&amp;\\mathrm{Cov}[Z_{t}+0.9 Z_{t-1}+ 0.5Z_{t-2},~Z_{t+2}+0.9 Z_{t+1}+ 0.5Z_{t}]\\nonumber\\\\ &amp;=&amp;0.5\\mathrm{Cov}[Z_{t},~Z_{t}]\\nonumber\\\\ &amp;=&amp;0.5\\sigma^{2}_{z}.\\nonumber \\end{eqnarray} \\] Therefore the autocovariance function is given by \\[\\gamma_{\\tau}=\\left\\{\\begin{array}{cc}2.06\\sigma^{2}_{z}&amp;\\mbox{if}~\\tau=0\\\\ 1.35\\sigma^{2}_{z}&amp;\\mbox{if}~\\tau=1\\\\ 0.5\\sigma^{2}_{z}&amp;\\mbox{if}~\\tau=2\\\\ 0&amp;\\tau&gt;2\\\\ \\end{array}\\right. \\] while the autocorrelation function is given by \\[\\rho_{\\tau}=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ 0.655&amp;\\mbox{if}~\\tau=1\\\\ 0.243&amp;\\mbox{if}~\\tau=2\\\\ 0&amp;\\tau&gt;2\\\\ \\end{array}\\right. \\] 7.4 Invertibility Therefore, given a set of sample autocorrelation functions \\(\\hat{\\boldsymbol{\\rho}}_p\\) calculated from a time series, there is a unique set of parameters $that best fit the data within the class of stationary AR(p) processes. For an MA(q) process the corresponding relationship is given by \\[\\rho_{\\tau}=\\mathrm{Corr}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}}{\\sum_{j=0}^{q}\\lambda_{j}^{2}}&amp;\\mbox{if}~\\tau=1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right. \\] which is a set of non-linear equations in \\(\\boldsymbol{\\lambda}=(\\lambda_{1},\\ldots,\\lambda_{q})\\). Therefore for a given autocorrelation function \\(\\boldsymbol{\\rho}_{q}\\) there may exist more than one set of parameters \\(\\lambda\\) that satisfy the above equation. 7.4.1 Inveritibility Theorem The MA(q) process \\[\\begin{eqnarray} X_{t}&amp;=&amp;Z_{t}+\\lambda_{1}Z_{t-1}+\\ldots+\\lambda_{q}Z_{t-q}\\nonumber\\\\ &amp;=&amp;(1+\\lambda_{1}B+\\ldots+\\lambda_{q}B^{q})Z_{t}\\nonumber\\nonumber\\\\ &amp;=&amp;\\theta(B)Z_{t}\\nonumber \\end{eqnarray} \\] is invertible if and only if the roots of the characteristic polynomial \\(\\theta (B)\\) have modulus greater than one and hence lie outside the unit circle. 7.4.1.1 Example of invertibility \\[X_{t}=Z_{t} + 4.25Z_{t-1} + Z_{t-2}\\] Characteristic polynomial is given by \\(\\theta(B)~=~1 + 4.25B + B^{2}\\) Solving \\(1 + 4.25B + B^{2}=0\\) gives: \\[\\begin{eqnarray} \\mbox{roots}&amp;=&amp;\\frac{-4.25\\pm\\sqrt{(4.25)^2-4\\times1\\times1}}{2\\times 1}\\nonumber\\\\ \\mbox{roots}&amp;=&amp;\\frac{-4.25\\pm3.75}{2}\\nonumber\\\\ \\mbox{roots}&amp;=&amp;-0.25\\mbox{ and }-4\\nonumber \\end{eqnarray} \\] Therefore as one of the routes has modulus less than one the process is not invertible. 7.5 MA Model Identification Determine if a MA model is appropriate and which order process should we use using the autocorrelation function ACF: \\[\\rho_{\\tau}=\\mathrm{Corr}[X_{t},X_{t+\\tau}]=\\left\\{\\begin{array}{cc}1&amp;\\mbox{if}~\\tau=0\\\\ \\frac{\\sum_{j=0}^{q-\\tau}\\lambda_{j}\\lambda_{j+\\tau}}{\\sum_{j=0}^{q}\\lambda_{j}^{2}}&amp;\\mbox{if}~\\tau=1,\\ldots,q\\\\ 0&amp;\\tau&gt;q\\\\ \\end{array}\\right. \\] This function is non-zero for \\(\\tau \\le q\\) and zero for \\(\\tau &gt;q\\) Note The autocorrelation function (ACF) tells us whether an MA(q) process is appropriate while the partial autocorrelation function (PACF) suggests whether an AR(p) process is appropriate. Therefore for a given time series, both should be plotted to show which process would be a good model. 7.5.1 Example Looking at ACF, Data 1 uses MA(1) process, Data 2 uses MA(2), Data 3 uses MA(3), Data 4 uses MA(12) 7.6 MA parameter estimation Conditional least squares: Consider the MA(1) model \\(X_{t}=\\mu+Z_{t}+\\lambda Z_{t-1}\\), where \\(\\mathrm{Var}[Z_{t}]=\\sigma^{2}_{z}\\). The conditional least squares algorithm works as follows: Select starting values for \\(\\mu\\) and \\(\\lambda\\), \\[\\tilde{\\mu}=\\frac{1}{n}\\sum_{t=1}^{n}x_{t}\\hspace{1cm}\\mbox{and by solving}\\hspace{1cm}\\hat{\\rho}_{1}=\\frac{\\tilde{\\lambda}}{1+\\tilde{\\lambda}^{2}} \\] Calculate the conditional residual sum of squares \\[S(\\tilde{\\mu},\\tilde{\\lambda})=\\sum_{t=1}^{n}[x_{t}-\\tilde{\\mu}-\\tilde{\\lambda}Z_{t-1}]^{2} \\] Where \\(Z_0=0\\) and \\(_Z_t\\) is calculated recursively using \\[Z_{t}=x_{t}-\\tilde{\\mu}-\\tilde{\\lambda}Z_{t-1} \\] Repeat step 2 for a range of values of \\((\\mu, \\lambda)\\) that are close to the initial estimates in step 1. Then determine the estimates \\((\\hat{\\mu},\\hat{\\lambda})\\) as the values that mimimize \\(S(\\tilde{\\mu},\\tilde{\\lambda})\\) over all those values considered Using the fact that the variance of an MA(1) process is \\(\\mathrm{Var}[X_{t}]=\\sigma^{2}_{z}(1+\\lambda^{2})\\), we obtain that \\[\\hat{\\sigma}^{2}_{z}=\\frac{\\hat{\\sigma}^{2}}{(1+\\hat{\\lambda}^{2})} \\] where \\(\\hat \\sigma^2\\) is the overall variance of the process. 7.7 MA Example # Generate MA(3) data with a quadratic trend time &lt;- 1:200 n &lt;- 200 sd &lt;- 10 data.ar &lt;- arima.sim(model = list(ma = c(0.6, 0.3, 0.6)), n = n, sd = sd) data &lt;- data.ar + 30 + 0.05 * time + 0.004*time^2 par(mfrow = c(2, 1)) plot(time, data, type = &quot;l&quot;, main = &quot;Raw data plot&quot;) acf(data, main = &quot;Sample autocorrelation function&quot;) From looking at the time plot and correlogram the data appear to have a quadratic trend, which we remove before trying to model the correlation. Fitting \\(m_{t}=\\beta_{0}+\\beta_{1}t+\\beta_{2}t^{2}\\) gives the follwing residual series: # Remove the trend time2 &lt;- time^2 linear.model &lt;- lm(data ~ time+time2) residual.series &lt;- data - linear.model$fitted.values par(mfrow = c(3,1)) plot(residual.series, main = &quot;Residual series&quot;) acf(residual.series, main = &quot;ACF of Residual series&quot;) pacf(residual.series, main = &quot;PACF of Residual series&quot;) Time plot suggests stationary while ACF suggest an MA(3) process is appropriate. Note that the PACF doesn’t tell us a lot here. Finally we fit a MA(3) model using function arima() in R # Fit an MA(3) model to the data model.ma &lt;- arima(residual.series, order = c(0, 0, 3)) model.ma ## ## Call: ## arima(x = residual.series, order = c(0, 0, 3)) ## ## Coefficients: ## ma1 ma2 ma3 intercept ## 0.6409 0.4573 0.6248 -0.1172 ## s.e. 0.0544 0.0665 0.0561 1.7578 ## ## sigma^2 estimated as 84.39: log likelihood = -728.25, aic = 1466.51 arima(x = residual.series, order = c(0, 0, 3)) ## ## Call: ## arima(x = residual.series, order = c(0, 0, 3)) ## ## Coefficients: ## ma1 ma2 ma3 intercept ## 0.6409 0.4573 0.6248 -0.1172 ## s.e. 0.0544 0.0665 0.0561 1.7578 ## ## sigma^2 estimated as 84.39: log likelihood = -728.25, aic = 1466.51 par(mfrow = c(3, 1)) plot(model.ma$residuals, main = &quot;MA(3) Residual series&quot;) acf(model.ma$residuals, main = &quot;MA(3) ACF of Residual series&quot;) pacf(model.ma$residuals, main = &quot;MA(3) PACF of Residual series&quot;) Residuals look independent (resemble white noise), so the model below is appropriate \\[X_{t}=\\beta_{0}+\\beta_{1}t+\\beta_{2}t^{2}+\\lambda_{1}Z_{t-1}+\\lambda_{2}Z_{t-2}+\\lambda_{3}Z_{t-3}+Z_{t}. \\] "],
["chapter-4-lab.html", "8 Chapter 4 Lab 8.1 Moving Average model (MA)", " 8 Chapter 4 Lab 8.1 Moving Average model (MA) \\[ X_t = \\mu + Noise_t + \\lambda \\cdot(Noise_{t-1})\\] Regression on today’s data using yesterday’s noise. # Generate MA model with slope 0.5 x &lt;- arima.sim(model = list(ma = 0.5), n = 100) # Generate MA model with slope 0.9 y &lt;- arima.sim(model = list(ma = 0.9), n = 100) # Generate MA model with slope -0.5 z &lt;- arima.sim(model = list(ma = -0.5), n = 100) # Plot all three models together plot.ts(cbind(x, y, z)) par(mfrow = c(3, 1)) # Calculate the ACF acf(x) acf(y) acf(z) The ACF plots tell that it is appropriate to use MA(1) model to fit this data. This is expected since we used MA(1) model with different values of \\(\\lambda\\) to simulate the data par(mfrow = c(3, 1)) pacf(x) pacf(y) pacf(z) "],
["chapter-5-more-general-time-series-processes.html", "9 Chapter 5: More general time series processes 9.1 ARMA model 9.2 ARIMA model", " 9 Chapter 5: More general time series processes For most data sets the AR and MA models will be adequate to represent short-term correlation, with autoregressive correlation occurring more often than moving average. However, occasionally you may meet data that are not well represented by either of these time series processes, an example of which is shown below. Example: Consider the following data which appear to be stationary but contain short-term correlation. The ACF and PACF suggest that neither an AR nor an MA process is appropriate, but as these are the only models we know, we fit them to the data to see how well they remove the short-term correlation. We chose the order (p and q) as the lowest values that removed the majority of the correlation, which resulted in an AR(6) model or an MA(5) model: Neither fits the data perfectly as the residual series isn’t white noise. Both also used high order processess (p=6 and q=5) which include a relatively large number of parameters. This emphasizes two points: Even if the correlation structure does not look like an AR(p) or an MA(q) process, fitting these models with large enough p and q will remove the majority of the correlation. Therefore it is better to model correlation with the wrong time series process than not to model it at all. However, AR(p) and MA(q) processes are not always appropriate models for short-term correlation 9.1 ARMA model An Autoregressive Moving Average process of order (p,q) denoted ARMA(p,q) is given by \\[\\begin{eqnarray} X_{t}&amp;=&amp;\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p} + Z_{t} + \\lambda_{1}Z_{t-1}+\\ldots+\\lambda_{q}Z_{t-q}\\nonumber\\\\ &amp;=&amp;\\sum_{j=1}^{p}\\alpha_{j}X_{t-j}+\\sum_{j=1}^{q}\\lambda_{j}Z_{t-j} + Z_{t}\\nonumber \\end{eqnarray} \\] Using Backshift operator the model can be rewritten as \\[\\phi(B)X_{t} = \\theta(B)Z_{t}\\] Example: The data in the first example can be modelled by an ARMA(1,1) process \\[X_{t}=\\alpha X_{t-1}+\\lambda Z_{t-1}+Z_{t} \\] 9.1.1 Mean of ARMA(p,q) process \\[E[X_t] = 0\\] 9.1.2 Variance and autocorrelation function Variance: \\[\\begin{eqnarray} \\mathrm{Var}[X_{t}]&amp;=&amp;\\alpha \\gamma_{1}+\\lambda (\\alpha\\sigma^{2}_{z} + \\lambda\\sigma^{2}_{z})+\\sigma^{2}_{z}\\nonumber\\\\ &amp;=&amp;\\alpha \\gamma_{1}+\\sigma^{2}_{z}(\\lambda^{2}+\\lambda\\alpha+1).\\nonumber \\end{eqnarray} \\] Autocorrelation function: \\[\\rho_{\\tau}=\\alpha \\rho_{\\tau-1}\\] 9.1.3 Stationarity and Invertibility We know that MA(q) process is always stationary and AR(p) process is always invertible. Therefore an ARMA(p,q) process is stationary if the AR(p) part is stationary, i.e., if the roots of the AR(p) characteristic polynomial have modulus larger than 1; and is invertible if the MA(q) part is invertible, i.e., if the roots of the MA(q) characteristic polynomial have modulus larger than 1 Example: Consider the ARMA(1,1) process \\[X_{t}=2X_{t-1}-0.4Z_{t-1}+Z_{t}\\] which can be re-written as \\[X_{t}(1-2B)=Z_{t}(1-0.4B)\\] AR part has root \\(B=0.5\\), therefore the process is not stationary. The MA part has root \\(B=2.5\\), therefore the process is invertible. 9.1.4 ARMA model identification AR and MA processes are straightforward to identify from ACF and PACF: If the ACF is significantly different from zero for only the first q lags (for small q), then an MA(q) model is appropriate. If the PACF is significantly different from zero for only the first lags (for small p), then an AR(p) model is appropriate. Take another look at the data in the first example: Neither the ACF or PACF give any clues as to the appropriate type of time series process. All they tell us is that it is not an AR(p) process or an MA(q) process Notes: Model identification for ARMA(p,q) process when p,q &gt; 0 is difficult First determine if the ACF and PACF resemble either an MA(q) or an AR(p) process If not then adopt a trial and error approach, starting with the simplest model (i.e., an ARMA(1,1)) and increasing the complexity until the correlation has been removed. 9.1.5 ARMA parameter estimation Not in this course. 9.1.6 ARMA simulation example Chapter 2 discussed methods of removing trends and seasonal variation from time series data, while chapters 3 to 5 have described how to model short-term correlation in a stationary time series. But is modelling correlation important or can we simply ignore it? We look at the consequences of ignoring correlation using simulation. Consider the simple linear trend model \\[X_{t}=\\beta_{0}+\\beta_{1}t+e_{t}\\] where \\(e_t\\) could be independent errors or correlated. The aim is to estimate the slope coefficient \\(\\beta_1\\) and produce a 95% confidence interval. Two natural questions you could ask: Effect of correlation structure of \\(e_t\\) on the estimate and CI of \\(\\beta_1\\) If \\(e_t\\) is correlated, can we allow for this correlation when we estimate \\(\\beta_1\\)? 9.1.6.1 Generate data: Time series of length 1000 from 2 models: A: \\(X_t=30+0.1t+Z_t\\) B: \\(X_t=30+0.1t+Y_t\\) where the regression parameter \\(\\beta=0.1\\) \\(Z_t\\) is a purely random process (white noise), meaning that model A is a linear trend with independent errors \\(Y_t\\) is an AR(1) process with lag one autocorrelation coefficient equal to 0.9, meaning that model is a linear trend with correlated errors We simulate 1000 sets of data from models A and B: n &lt;- 1000 sd &lt;- 10 corr.ar1 &lt;- arima.sim(model = list(ar = c(0.9)), n = n, sd = sd) corr.indep &lt;- arima.sim(model = list(), n = n, sd = sd)[1:n] time &lt;- 1:n data.ar1 &lt;- corr.ar1 + 30 + 0.1*time data.indep &lt;- corr.indep + 30 + 0.1*time 9.1.6.2 Measure model quality From 1000 simulated data points we get 1000 estimates of \\(\\beta, \\hat \\beta_1,...,\\hat\\beta_{1000}\\). Using 3 standard metrics to determine how good they are: Bias - On average how different is \\(\\beta\\) from the estimates, which is calculated as \\[\\mbox{Bias}(\\beta)=\\mathbb{E}[\\hat{\\beta}]-\\beta~=~\\frac{1}{1000}\\sum_{j=1}^{1000}\\hat{\\beta}_{j} - \\beta. \\] Root mean square error (RMSE) - How much variation is there between 1000 estimates: \\[\\mbox{RMSE}(\\beta)=\\sqrt{\\mathbb{E}[(\\hat{\\beta}-\\beta)^{2}]}~=~\\sqrt{\\frac{1}{1000}\\sum_{j=1}^{1000}(\\hat{\\beta}_{j} - \\beta)^{2}}. \\] Coverage probability - Each data set produces an estimate and 95% CI for \\(\\beta\\). What percentage of the 95% CI contain the true value \\(\\beta\\) 9.1.6.3 Conduct the simulation study We answer the questions listed above: Effect of correlation structure of \\(e_t\\) on the estimate and CI of \\(\\beta_1\\)? Naively assume that both data sets are independent, and estimate \\(\\beta_1\\) and its 95% confidence interval using the lm() function. Save the estimates and 95% CI from each data set and calculate the bias, RMSE, and coverage probability. If \\(e_t\\) is correlated, can we allow for this correlation when we estimate \\(\\beta_1\\)? Simultaneously estimate the linear trend and model the correlation using the arima() function. The R code to do this is as follows: model.ar1 &lt;- lm(data.ar1 ~ time) arima(data.ar1, order = c(1, 0, 0), xreg = time) ## ## Call: ## arima(x = data.ar1, order = c(1, 0, 0), xreg = time) ## ## Coefficients: ## ar1 intercept time ## 0.8745 29.0048 0.1034 ## s.e. 0.0154 4.9940 0.0086 ## ## sigma^2 estimated as 101.4: log likelihood = -3729.39, aic = 7466.78 The following strategy is appropriate for modelling trend and correlation in time series data using regression methods: First remove any trend and seasonal variation assuming the observation are independent using the lm() function, since at this stage we do not know if there is any correlation Determine whether the residuals have any short-term correlation, if so what type of stationary time series model is appropriate Finally, simultaneously estimate the correlation and trend using the arima() function 9.2 ARIMA model An alternative approach to model trend, seasonal variation, and correlation simultaneously (instead of modelling trend and seasonality first, then model the residuals with a short-term correlation model of AR(p) or MA(q)) is ARIMA(p,d,q). This model is not appropriate if the goal is to capture the shape of the trend or seasonal variation. ARIMA(p,d,q) combines ARMA(p,q) and differencing by order differences of d (difference d times) 9.2.1 Definition \\(\\{X_t\\}\\) is an Autoregressive Integrated Moving Average model of order (p,d,q), denoted ARIMA(p,d,q) if the \\(d^{th}\\) order differenced process \\[Y_t = \\nabla^d X_t =(1-B)^d X_t \\] is an ARMA(p,q) process. An ARIMA(p,d,q) process can be written most easily in terms of characteristic polynomials. If we write the ARMA(p,q) process for \\(Y_t\\) as \\(\\phi(B)Y_t=\\theta(B)Z_t\\) then as \\(Y_{t}=(1-B)^d X_t\\), an ARIMA(p,d,q) can be written as \\[\\phi(B) (1-B)^d X_t = \\theta(B) Z_t\\] Notes: The characteristic polynomial for the AR part of the ARIMA model is equal to \\(\\phi^*(B) = \\phi(B) (1-B)^d\\) which has d roots equal 1. Thus an ARIMA process cannot be stationary unless d = 0 ARIMA(p,0,q) = ARMA(p,q) Examples: ARIMA(0,1,0) is a random walk process. Note that AR(1) is also a random walk process if \\(\\alpha=1\\): \\[\\begin{eqnarray} (1-B)X_{t}&amp;=&amp;Z_{t}\\nonumber\\\\ X_{t}-X_{t-1}&amp;=&amp;Z_{t}\\nonumber\\\\ X_{t}&amp;=&amp;X_{t-1}+Z_{t}\\nonumber \\end{eqnarray}\\] ARIMA(0,0,0) is a purely random process: \\[\\begin{eqnarray} (1-B)^{0}X_{t}&amp;=&amp;Z_{t}\\nonumber\\\\ X_{t}&amp;=&amp;Z_{t}\\nonumber \\end{eqnarray}\\] ARIMA(1,1,1) process has characteristic polynomials \\(\\phi(B)=1-\\alpha B\\) and \\(\\theta(B)=1+\\lambda B\\), meaning the full model is given by \\[\\begin{eqnarray} (1-\\alpha B)(1-B)X_{t}&amp;=&amp;(1+\\lambda B)Z_{t}\\nonumber\\\\ (1-B-\\alpha B + \\alpha B^{2})X_{t}&amp;=&amp;\\lambda Z_{t-1}+Z_{t}\\nonumber\\\\ X_{t}&amp;=&amp;(1+\\alpha)X_{t-1}-\\alpha X_{t-2} +\\lambda Z_{t-1}+Z_{t}.\\nonumber \\end{eqnarray}\\] So ARIMA(1,1,1) model is essentially a non-stationary ARMA(2,1) model 9.2.2 ARIMA model identification Choose \\(d\\) - Plot the time series and its correlogram and determine whether data contain a trend. If there is no trend then \\(d=0\\), otherwise increase \\(d\\) by 1 and repeat this step until the process is stationary Choose \\(p\\) and \\(q\\) - Plot the ACF and PACF of the \\(d^{th}\\) order differences, and determine the apporiate ARMA(p,q) model Estimate the parameters - Use the ARIMA() function in R to estimate the parameters of the ARIMA process. Residual diagnosis - Plot the timeplot, ACF and PACF of the residuals and determine whether they contain any remaining trend, seasonal variation or short-term correlation. A test for correlation such as the one based on the Ljung-Box statistic could also be used. If the residuals resemble a purely random process (white noise) then stop, otherwise return to stage one and change either p, d, or q. 9.2.3 Example 1 Below are the realisations of an ARIMA(1,1,0) (left) and ARIMA(0,1,1) (right) processes. Data are non-stationary and have a trend, there fore the ACF is not informative regarding the presence of absence of short-term correlation. # Simulate ARIMA(1,1,0) and ARIMA(0,1,1) data n &lt;- 1000 sd &lt;- 1 data1 &lt;- arima.sim(model = list(ar = c(0.7), order = c(1, 1, 0)), n = n, sd = sd) data2 &lt;- arima.sim(model = list(ma = c(0.7), order = c(0, 1, 1)), n = n, sd = sd) par(mfrow = c(3, 2)) plot(data1, main = &quot;ARIMA(1,1,0) data&quot;) plot(data2, main = &quot;ARIMA(0,1,1) data&quot;) acf(data1, main = &quot;ARIMA(1,1,0) ACF&quot;) acf(data2, main = &quot;ARIMA(0,1,1) ACF&quot;) pacf(data1, main = &quot;ARIMA(1,1,0) PACF&quot;) pacf(data2, main = &quot;ARIMA(0,1,1) PACF&quot;) 9.2.4 Example 2 Daily respiratory admissions data for Glasgow between 2000 and 2007. Here we model the trend and correlation using ARIMA(p,d,q) model. First order difference plot appear to be stationary with mean of zero, meaning that d = 1 is adequate. The ACF and PACF of the first order differences show that the differencing process has induced negative correlation into the data. An MA(1) process appears to be an appropriate model so we fit an ARIMA(0,1,1) to the original data. This provides the following residuals shown below These residuals appear to resemble a purely random process with no correlation, so the ARIMA(0,1,1) model appears to be appropriate. "],
["chapter-5-lab.html", "10 Chapter 5 Lab 10.1 Oil Return 10.2 Air Passengers 10.3 Birth Rate", " 10 Chapter 5 Lab 10.1 Oil Return An example using ARMA(1,1) process: library(astsa) plot(oil) # Calculate approximate oil returns oil_returns &lt;- diff(log(oil)) # Plot oil_returns. Notice the outliers. plot(oil_returns) # Plot the P/ACF pair for oil_returns acf2(oil_returns) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## ACF 0.13 -0.07 0.13 -0.01 0.02 -0.03 -0.03 0.13 0.08 0.02 0.01 0 -0.02 ## PACF 0.13 -0.09 0.16 -0.06 0.05 -0.08 0.00 0.12 0.05 0.03 -0.02 0 -0.03 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## ACF 0.06 -0.05 -0.09 0.03 0.05 -0.05 -0.07 0.04 0.09 -0.05 -0.08 -0.07 ## PACF 0.09 -0.07 -0.06 0.01 0.04 -0.05 -0.05 0.05 0.06 -0.06 -0.05 -0.08 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## ACF 0.00 -0.11 -0.07 0.02 -0.02 -0.03 -0.05 -0.03 0.00 -0.09 -0.01 -0.04 ## PACF 0.02 -0.11 0.01 0.00 -0.01 -0.05 -0.04 0.02 0.02 -0.08 0.02 -0.04 ## [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## ACF -0.01 0.02 -0.01 -0.06 0.01 0.00 -0.01 0.04 0.01 0.05 0.07 -0.01 ## PACF 0.04 -0.01 -0.01 -0.05 0.03 -0.03 0.00 0.08 0.00 0.05 0.01 0.04 ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61] ## ACF -0.03 0.01 -0.04 -0.04 -0.03 0 -0.01 -0.10 -0.01 -0.05 -0.04 -0.03 ## PACF -0.08 0.01 -0.07 0.00 -0.06 0 -0.06 -0.11 0.01 -0.09 -0.01 -0.04 ## [,62] [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72] [,73] ## ACF 0.01 0.01 -0.01 -0.04 0.02 0 -0.01 -0.03 -0.02 -0.05 -0.01 -0.01 ## PACF 0.04 -0.01 0.00 -0.04 0.03 0 0.00 -0.04 -0.02 -0.04 0.00 -0.01 ## [,74] [,75] [,76] [,77] [,78] [,79] [,80] [,81] [,82] [,83] [,84] [,85] ## ACF -0.02 0.01 0.02 0.04 -0.01 0.03 0.02 -0.04 -0.01 0.02 0.03 0.01 ## PACF 0.00 0.02 -0.01 0.04 -0.02 0.08 -0.03 -0.03 -0.03 0.03 -0.03 -0.02 ## [,86] [,87] [,88] [,89] [,90] [,91] [,92] [,93] [,94] [,95] [,96] [,97] ## ACF 0.03 0.08 -0.04 -0.02 0.01 -0.04 0.05 0.07 -0.04 0.02 0.05 0.01 ## PACF 0.03 0.04 -0.09 -0.01 -0.02 -0.03 0.03 0.05 -0.11 0.02 -0.01 0.02 ## [,98] [,99] [,100] [,101] [,102] [,103] [,104] [,105] [,106] [,107] [,108] ## ACF 0.00 0.01 0.04 0.01 -0.03 -0.04 -0.01 0.02 0.01 0.01 0.06 ## PACF -0.03 0.06 0.01 -0.05 0.02 -0.03 0.01 0.00 0.04 -0.01 0.07 ## [,109] [,110] [,111] [,112] [,113] [,114] [,115] [,116] [,117] [,118] ## ACF 0.08 0.04 0.02 0.01 0.03 0.02 -0.02 -0.04 -0.01 0.04 ## PACF 0.04 0.04 0.00 0.05 -0.01 0.00 -0.04 -0.03 -0.03 0.02 ## [,119] [,120] [,121] [,122] [,123] [,124] [,125] [,126] [,127] [,128] ## ACF 0.05 -0.02 -0.02 0.03 0.01 -0.04 -0.08 0.02 0.00 -0.04 ## PACF 0.04 -0.01 -0.04 -0.01 0.03 -0.03 -0.07 0.00 -0.02 -0.04 ## [,129] [,130] [,131] [,132] [,133] [,134] [,135] [,136] [,137] [,138] ## ACF 0.01 0.02 0.01 0.02 0.00 -0.01 0.00 -0.03 -0.06 0.01 ## PACF 0.01 0.01 -0.01 0.02 0.05 0.02 0.01 0.02 -0.02 0.04 ## [,139] [,140] [,141] [,142] [,143] [,144] [,145] [,146] [,147] [,148] ## ACF -0.02 -0.02 0.02 -0.01 -0.03 0.00 0.00 -0.04 -0.01 -0.02 ## PACF 0.01 -0.03 -0.02 0.02 -0.01 0.02 -0.01 0.02 -0.02 -0.03 ## [,149] [,150] [,151] [,152] [,153] [,154] [,155] [,156] [,157] [,158] ## ACF -0.04 -0.04 0.01 0.01 0.04 0.03 0.01 0.05 0.01 -0.06 ## PACF -0.02 -0.01 0.02 -0.01 0.04 0.03 -0.04 0.03 0.00 -0.05 ## [,159] [,160] [,161] [,162] [,163] [,164] [,165] [,166] [,167] [,168] ## ACF 0.02 0.05 -0.02 0.05 0.00 -0.01 0 -0.01 -0.02 -0.01 ## PACF 0.02 0.03 0.00 0.03 0.01 0.00 0 0.02 0.01 -0.01 ## [,169] [,170] [,171] [,172] [,173] [,174] [,175] [,176] [,177] [,178] ## ACF 0.00 -0.03 -0.01 -0.02 -0.02 0.04 -0.01 -0.03 0.02 0.01 ## PACF 0.03 -0.03 0.00 -0.04 0.00 0.02 -0.03 -0.01 0.01 0.02 ## [,179] [,180] [,181] [,182] [,183] [,184] [,185] [,186] [,187] [,188] ## ACF -0.01 -0.01 -0.04 0.07 -0.01 -0.04 0.05 -0.02 -0.01 0.01 ## PACF -0.01 0.00 -0.04 0.08 -0.05 0.02 -0.01 -0.02 0.03 0.00 ## [,189] [,190] [,191] [,192] [,193] [,194] [,195] [,196] [,197] [,198] ## ACF -0.05 -0.04 -0.01 0.01 0.04 -0.01 0.00 0.06 -0.06 -0.02 ## PACF -0.03 -0.04 0.01 -0.01 0.07 -0.01 0.02 -0.01 -0.03 0.01 ## [,199] [,200] [,201] [,202] [,203] [,204] [,205] [,206] [,207] [,208] ## ACF 0.02 0 0.00 0.00 -0.04 0.00 0.04 0.04 0.04 0.01 ## PACF 0.00 0 -0.02 -0.01 -0.05 -0.01 0.02 0.04 0.05 -0.01 # Assuming both P/ACF are tailing, fit a model sarima(oil_returns, p = 1, d = 0, q = 1) ## initial value -3.057594 ## iter 2 value -3.061420 ## iter 3 value -3.067360 ## iter 4 value -3.067479 ## iter 5 value -3.071834 ## iter 6 value -3.074359 ## iter 7 value -3.074843 ## iter 8 value -3.076656 ## iter 9 value -3.080467 ## iter 10 value -3.081546 ## iter 11 value -3.081603 ## iter 12 value -3.081615 ## iter 13 value -3.081642 ## iter 14 value -3.081643 ## iter 14 value -3.081643 ## iter 14 value -3.081643 ## final value -3.081643 ## converged ## initial value -3.082345 ## iter 2 value -3.082345 ## iter 3 value -3.082346 ## iter 4 value -3.082346 ## iter 5 value -3.082346 ## iter 5 value -3.082346 ## iter 5 value -3.082346 ## final value -3.082346 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 xmean ## -0.5264 0.7146 0.0018 ## s.e. 0.0871 0.0683 0.0022 ## ## sigma^2 estimated as 0.002102: log likelihood = 904.89, aic = -1801.79 ## ## $degrees_of_freedom ## [1] 541 ## ## $ttable ## Estimate SE t.value p.value ## ar1 -0.5264 0.0871 -6.0422 0.0000 ## ma1 0.7146 0.0683 10.4699 0.0000 ## xmean 0.0018 0.0022 0.7981 0.4252 ## ## $AIC ## [1] -3.312109 ## ## $AICc ## [1] -3.312027 ## ## $BIC ## [1] -3.280499 Oil price is hard to model since there is a 2008 peak period for oil price followed by a sharp drop due to the financial crisis. Taking the first order difference yields the daily return of oil, but the data is not quite stationary. The ACF and PACF suggests an ARMA(1,1) model, which I fitted to the differenced series. The residuals does not look like white noise and failed the Q-Q plot and Ljung-Box statistic. 10.2 Air Passengers # Load data library(astsa) plot(AirPassengers) From the plot of Air Passengers, there are: trend, seasonal variation, heteroscedasticity (variance increases over time). 10.3 Birth Rate plot(birth) # Plot P/ACF to lag 60 of differenced data d_birth &lt;- diff(birth) acf2(d_birth, max.lag = 60) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## ACF -0.32 0.16 -0.08 -0.19 0.09 -0.28 0.06 -0.19 -0.05 0.17 -0.26 0.82 ## PACF -0.32 0.06 -0.01 -0.25 -0.03 -0.26 -0.17 -0.29 -0.35 -0.16 -0.59 0.57 ## [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] ## ACF -0.28 0.17 -0.07 -0.18 0.08 -0.28 0.07 -0.18 -0.05 0.16 -0.24 0.78 ## PACF 0.13 0.11 0.13 0.09 0.00 0.00 0.05 0.04 -0.07 -0.10 -0.20 0.19 ## [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] ## ACF -0.27 0.19 -0.08 -0.17 0.07 -0.29 0.07 -0.15 -0.04 0.14 -0.24 0.75 ## PACF 0.01 0.05 0.07 0.07 -0.02 -0.06 -0.02 0.09 0.03 -0.06 -0.16 0.03 ## [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] ## ACF -0.23 0.16 -0.08 -0.15 0.05 -0.25 0.06 -0.18 -0.03 0.15 -0.22 0.72 ## PACF 0.08 -0.10 -0.03 0.07 -0.04 0.06 0.04 -0.07 -0.06 0.02 -0.04 0.10 ## [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] ## ACF -0.24 0.16 -0.08 -0.13 0.05 -0.26 0.05 -0.17 -0.02 0.15 -0.23 0.70 ## PACF 0.01 0.00 -0.03 0.04 0.03 0.00 -0.01 0.01 0.03 0.04 -0.09 0.04 # Plot P/ACF to lag 60 of seasonal differenced data dd_birth &lt;- diff(d_birth, lag = 12) acf2(dd_birth, max.lag = 60) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## ACF -0.3 -0.09 -0.09 0.00 0.07 0.03 -0.07 -0.04 0.11 0.04 0.13 -0.43 0.14 ## PACF -0.3 -0.20 -0.21 -0.14 -0.03 0.02 -0.06 -0.08 0.06 0.08 0.23 -0.32 -0.06 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## ACF -0.01 0.03 0.01 0.02 0.00 0.03 -0.07 -0.01 0 0.06 -0.01 -0.12 ## PACF -0.13 -0.13 -0.11 0.02 0.06 0.04 -0.10 0.02 0 0.17 -0.13 -0.14 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## ACF 0.17 -0.04 0.03 -0.05 -0.09 -0.01 0.19 -0.03 -0.09 -0.02 -0.04 0.17 ## PACF 0.07 -0.04 -0.02 0.02 -0.06 -0.07 0.05 0.07 -0.06 0.05 -0.16 -0.01 ## [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## ACF -0.14 0.03 -0.05 0.03 0.10 0 -0.10 -0.03 0.06 0.02 0.01 -0.01 ## PACF -0.04 -0.01 -0.03 -0.01 0.01 0 0.03 -0.02 -0.07 0.05 -0.11 0.05 ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] ## ACF 0.06 -0.08 0.03 0.01 -0.02 -0.01 0.00 -0.07 0.17 -0.04 -0.01 ## PACF 0.06 -0.03 -0.03 0.04 0.02 -0.04 -0.01 -0.13 0.07 0.07 -0.05 # Fit SARIMA(0,1,1)x(0,1,1)_12. What happens? sarima(birth, p = 0, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12) ## initial value 2.219164 ## iter 2 value 2.013310 ## iter 3 value 1.988107 ## iter 4 value 1.980026 ## iter 5 value 1.967594 ## iter 6 value 1.965384 ## iter 7 value 1.965049 ## iter 8 value 1.964993 ## iter 9 value 1.964992 ## iter 9 value 1.964992 ## iter 9 value 1.964992 ## final value 1.964992 ## converged ## initial value 1.951264 ## iter 2 value 1.945867 ## iter 3 value 1.945729 ## iter 4 value 1.945723 ## iter 5 value 1.945723 ## iter 5 value 1.945723 ## iter 5 value 1.945723 ## final value 1.945723 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), include.mean = !no.constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 sma1 ## -0.4734 -0.7861 ## s.e. 0.0598 0.0451 ## ## sigma^2 estimated as 47.4: log likelihood = -1211.28, aic = 2428.56 ## ## $degrees_of_freedom ## [1] 358 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.4734 0.0598 -7.9097 0 ## sma1 -0.7861 0.0451 -17.4227 0 ## ## $AIC ## [1] 6.545975 ## ## $AICc ## [1] 6.546062 ## ## $BIC ## [1] 6.577399 # Fit another model, this time with an AR sarima(birth, p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12) ## initial value 2.218186 ## iter 2 value 2.032584 ## iter 3 value 1.982464 ## iter 4 value 1.975643 ## iter 5 value 1.971721 ## iter 6 value 1.967284 ## iter 7 value 1.963840 ## iter 8 value 1.961106 ## iter 9 value 1.960849 ## iter 10 value 1.960692 ## iter 11 value 1.960683 ## iter 12 value 1.960675 ## iter 13 value 1.960672 ## iter 13 value 1.960672 ## iter 13 value 1.960672 ## final value 1.960672 ## converged ## initial value 1.940459 ## iter 2 value 1.934425 ## iter 3 value 1.932752 ## iter 4 value 1.931750 ## iter 5 value 1.931074 ## iter 6 value 1.930882 ## iter 7 value 1.930860 ## iter 8 value 1.930859 ## iter 8 value 1.930859 ## final value 1.930859 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), include.mean = !no.constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 ma1 sma1 ## 0.3038 -0.7006 -0.8000 ## s.e. 0.0865 0.0604 0.0441 ## ## sigma^2 estimated as 45.91: log likelihood = -1205.93, aic = 2419.85 ## ## $degrees_of_freedom ## [1] 357 ## ## $ttable ## Estimate SE t.value p.value ## ar1 0.3038 0.0865 3.5104 5e-04 ## ma1 -0.7006 0.0604 -11.5984 0e+00 ## sma1 -0.8000 0.0441 -18.1302 0e+00 ## ## $AIC ## [1] 6.522519 ## ## $AICc ## [1] 6.522695 ## ## $BIC ## [1] 6.564418 "],
["chapter-6-forecasting.html", "11 Chapter 6: Forecasting 11.1 General problem 11.2 Regression 11.3 Exponential smoothing 11.4 Forecasting from AR(p) models 11.5 Forecasting from MA(q) models 11.6 Forecasting time series with trend, seasonality and correlation", " 11 Chapter 6: Forecasting Predicting the value of a time series at future points in time. The majority of forecasting methods are based on a statistical model, so if the model is not appropriate, then the forecasts will be useless. Even if an appropriate model is fitted, it does not mean the forecasts will be reasonable. However, despite these reservations forecasting is vitally important, and is often the sole goal of a time series analysis. 11.1 General problem A time series has been observed at \\(n\\) time points and predictions are required for the series at time \\(n+1, n+2\\), etc. We denote the \\(k\\) steps ahead forecast of \\(x_{n+k}\\) given data \\((x_1,...,x_n)\\) by \\(x_n(k)\\), so that \\(x_n(1)\\) is the prediction of \\(x_{n+1}\\) based on data up to and including time \\(n\\) Forecast error is given by \\[e_{n}(k)=x_{n+k}-x_{n}(k)\\] and is the amount by which the forecast differs from the true observation (once it has become available). The amount of uncertainty in a forecast is measured by the size of its error variance, \\(\\mathrm{Var}[e_{n}(k)\\), with larger values meaning the forecast is less reliable. To evaluate the performance of a forecasting method on a given data set, we calculate 1 step ahead forecasts \\(x_{1}(1),\\ldots, x_{n-1}(1)\\) and measure the discrepancy to the observed values \\(x_{2},\\ldots,x_{n}\\) using the root mean square prediction error \\[\\mbox{RMSPE}=\\sqrt{\\frac{1}{n-1}\\sum_{k=1}^{n-1}e_{k}(1)^{2}}=\\sqrt{\\frac{1}{n-1}\\sum_{k=1}^{n-1}(x_{k+1}-x_{k}(1))^{2}}\\] We will compare three methods of forecasting: regression, exponential smoothing and ARIMA models. 11.2 Regression One approach is to ignore the temporal correlation in the observed data, and predict the next value of the time series based on linear regression methods. This method will only produce good forecasts if the time series being predicted has a strong trend and seasonal component compared to the amount of random variation and short-term correlation Additive time series model \\(X_{t}=m_{t}+s_{t}+e_{t}\\) the trend and seasonal variation are represented by \\(m_{t}+s_{t}=\\textbf{z}_{t}^\\top\\boldsymbol{\\beta};\\). The 1 step ahead prediction is then given by \\[x_{n}(1)=\\mathbf{z}_{n+1}^\\top\\hat{\\boldsymbol{\\beta}}\\] where \\(\\beta\\) is the cector of regression parameter estimates and \\(\\mathbf{z}_{n+1}^\\top\\) are the covariate values at time \\(n+1\\). Approximate 95% prediction intervals can be calculated from linear model theory as \\[\\mathbf{z}_{n+1}^\\top\\hat{\\boldsymbol{\\beta}}\\pm 1.96\\sqrt{\\hat{\\sigma}^{2} (1+\\mathbf{z}_{n+1}^\\top(Z^\\top Z)^{-1}\\mathbf{z}_{n+1})}\\] where \\(\\hat{\\sigma}^{2}\\) is the estimated residual variance from the linear model, and \\(Z\\) is the matrix of regression variables for all \\(n\\) time points. 11.2.1 Example 1 Consider the daily respiratory admissions data presented in Chapter 2. One of the trend models for these data was \\[X_{t}=\\beta_{0}+\\beta_{1}t+\\beta_{2}\\sin(2\\pi t/365)+\\beta_{3}\\cos(2\\pi t /365) + e_{t}\\] which modelled the regular seasonal pattern with a period of a year and a linear trend. Figure 6.1 shows the last year of these data, together with predictions for the first 100 days in 2008 using the model above. Note in this case how wide the prediction intervals are, because the seasonal pattern is overwhelmed by random variation. Thus the predictions are not likely to be very accurate. 11.2.2 Example 2 Consider air traffic data, which can be modelled using a quarterly seasonal indicator variable and a linear trend. Note that in this case the data are dominated by trend and seasonal variation, and have relatively little unexplained variation. Therefore the prediction intervals are narrow, suggesting the predictions will be fairly accurate. 11.2.3 Example 3 Consider the following linear trend model \\[X_t=10+2t+\\epsilon_t\\hspace{1cm}t=1,\\ldots,10\\] where \\(\\hat{\\boldsymbol{\\beta}}=(10,2)\\) and \\(\\hat\\sigma^2=1\\). The design matrix is given by \\[Z=\\left(\\begin{array}{cc} 1&amp; 1\\\\ 1&amp;2\\\\ \\vdots&amp;\\vdots\\\\ 1&amp;10\\\\ \\end{array}\\right)\\] Calculate a prediction and 95% vinterval for time 11 where \\(\\mathbf{z}_{11}^\\top=(1,11)\\) The prediction is given by \\[x_{10}(1)~=~\\mathbf{z}_{11}^\\top\\hat{\\boldsymbol{\\beta}}~=~ 1 \\times 10 + 11 \\times 2~=~32\\] To calculate the standard error we have that \\[Z^\\top Z=\\left(\\begin{array}{cc}10 &amp;55\\\\55 &amp; 385\\end{array}\\right) \\hspace{2cm}\\mbox{and}\\hspace{2cm} (Z^\\top Z)^{-1}=\\left(\\begin{array}{cc}0.46667 &amp;-0.06667\\\\-0.06667 &amp; 0.01212\\end{array}\\right)\\] Therefore we have that \\[\\mathrm{Var}[x_{10}(1)]~=~\\hat{\\sigma}^{2}(1+\\mathbf{z}_{11}^\\top (Z^\\top Z)^{-1}\\mathbf{z}_{11})=1.4665\\] and the 95% CI \\[\\begin{eqnarray} \\mathbf{z}_{n+1}^\\top\\hat{\\boldsymbol{\\beta}}&amp;\\pm&amp; 1.96\\sqrt{\\hat{\\sigma}^{2} (1+\\mathbf{z}_{n+1}^\\top (Z^\\top Z)^{-1}\\mathbf{z}_{n+1})}\\nonumber\\\\ 32 &amp;\\pm&amp; 1.96\\times \\sqrt{1.4665}\\nonumber\\\\ 32 &amp;\\pm&amp; 2.374\\nonumber\\\\ (29.63&amp;,&amp;34.37)\\nonumber \\end{eqnarray} \\] 11.3 Exponential smoothing Exponential smoothing is a simple procedure that does not assume a parametric model for the data, and is similar to moving average smoothing discussed in Chapter 2. It makes one step ahead forecasts of the form \\[\\hat{x}_{n}(1)=c_{0}x_{n}+c_{1}x_{n-1}+c_{2}x_{n-2}+\\ldots+c_{n-1}x_{1}\\] where the coefficient \\((c_{0},\\ldots,c_{n-1})\\) are called weights and must sum to one so that the prediction is of the correct size. In addition, the weights decrease as the observations move further away from the time point being predicted, i.e. \\(c_{0}\\geq c_{1} \\geq\\ldots\\geq c_{n-1}\\) 11.3.1 Definition Given data \\((x_{1},\\ldots,x_{n})\\) the one step ahead forecast using exponential smoothing is given by \\[\\hat{x}_{n}(1)=\\alpha x_{n}+\\alpha(1-\\alpha)x_{n-1}+\\alpha(1-\\alpha)^{2}x_{n-2}+\\ldots+\\alpha(1-\\alpha)^{n-1}x_{1}\\] where \\(\\alpha\\in[0,1]\\) is a smoothing parameter Notes: If \\(\\alpha\\) is close to 1, predictions are based on only the last few observations If \\(\\alpha\\) is close to 0, predictions are based on a large number of previous observations The one step ahead forecast can be written recursively as follows \\[\\begin{eqnarray} \\hat{x}_{n}(1)&amp;=&amp;\\alpha x_{n}+\\alpha(1-\\alpha)x_{n-1}+\\alpha(1-\\alpha)^{2}x_{n-2}+\\ldots+\\alpha(1-\\alpha)^{n-1}x_{1}\\nonumber\\\\ &amp;=&amp;\\alpha x_{n}+(1-\\alpha)[\\alpha x_{n-1}+\\alpha(1-\\alpha)x_{n-2}+\\ldots+\\alpha(1-\\alpha)^{n-2}x_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha x_{n}+(1-\\alpha)\\hat{x}_{n-1}(1)\\nonumber \\end{eqnarray}\\] making it straightforward computationally to update the forecasts in light of new data. To start the process, we set \\(\\hat{x}_{1}(1)=x_{2}\\) 11.3.2 Choosing \\(\\alpha\\) Choose \\(\\alpha\\) that minimizes the root mean square prediction error (RMSPE) In other words, for each candidate value of \\(\\alpha\\): Calculate \\(\\hat{x}_{1}(1),\\ldots,\\hat{x}_{n-1}(1)\\) using the recursive formula described above Calculate the RMSPE \\[\\mbox{RMSPE}=\\sqrt{\\frac{1}{n-1}\\sum_{k=1}^{n-1}(x_{k+1}-x_{k}(1))^{2}}\\] and choose the value of \\(\\alpha\\) that minimizes this quantity 11.3.3 Measure uncertainty For exponential smoothing it has been shown that an approximate 95% prediction interval for \\(x_{n}(1)\\) is given by \\[\\hat{x}_{n}(1)\\pm1.96\\sqrt{\\mathrm{Var}[e_{n}(1)]} \\] where \\(\\mathrm{Var}[e_{n}(1)]\\) can be approximated as the variance of the forecast errors \\(e_{1}(1),e_{2}(1),\\ldots,e_{n-1}(1)\\), i.e. \\[\\mathrm{Var}[e_{n}(1)]=\\frac{1}{n-2}\\sum_{i=1}^{n-1}(e_{i}(1)-\\bar{e})^{2} \\] with \\(\\bar{e}=\\sum_{i=1}^{n-1}e_{i}(1)/(n-1)\\) 11.3.3.1 Example Given the time series \\(\\mathbf{x}=(1,2,4,4,6,5,7,9,9,10)\\), calculate \\(x_{10}(1)\\) using only the last 5 observations for \\(\\alpha=0.1, 0.5, 0.9\\) \\[\\alpha=0.5\\hspace{0.5cm}x_{10}(1)=0.5\\times 10 + 0.5(1-0.5)\\times 9 + 0.5(1-0.5)^{2}\\times 9 + 0.5(1-0.5)^{3}\\times 7\\\\ + 0.5(1-0.5)^{4}\\times 5~=~8.89 \\] Note that as we are only using the last 5 observations to compute the forecasts, the prediction when \\(\\alpha=0.1\\) is artificially low 11.4 Forecasting from AR(p) models Assume the time series predicted is stationary with zero mean, as any trend or seasonal variation can be predicted using the regression methods described above. 11.4.1 AR(1) forecasting AR(1) process \\(X_{t}=\\alpha X_{t-1}+Z_{t}\\). Then the one step ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(1)&amp;=&amp;\\mathbb{E}[X_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\alpha X_{n}+Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha \\mathbb{E}[X_{n}|X_{n},X_{n-1},\\ldots,X_{1}]+\\mathbb{E}[Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha x_{n}\\nonumber \\end{eqnarray}\\] where \\(x_n\\) is the observed value of the series at time \\(n\\) and \\(\\mathbb{E}[Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]=0\\). The two steps ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(2)&amp;=&amp;\\mathbb{E}[X_{n+2}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\alpha X_{n+1}+Z_{n+2}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha \\mathbb{E}[X_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]+\\mathbb{E}[Z_{n+2}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha^{2} x_{n}.\\nonumber \\end{eqnarray}\\] Iterating the above procedure gives the \\(k\\) steps ahead forecast as \\[\\hat{x}_{n}(k)=\\alpha^{k}x_{n}\\] The forecast error variance at one step ahead is given by \\[\\begin{eqnarray} \\mathrm{Var}[e_{n}(1)]&amp;=&amp;\\mathrm{Var}[X_{n+1}-\\hat{x}_{n}(1)]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[\\alpha X_{n}+Z_{n+1} -\\alpha X_{n}]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[Z_{n+1}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}\\nonumber \\end{eqnarray} \\] and at two steps ahead it is \\[\\begin{eqnarray} \\mathrm{Var}[e_{n}(2)]&amp;=&amp;\\mathrm{Var}[X_{n+2}-\\hat{x}_{n}(2)]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[\\alpha X_{n+1}+Z_{n+2} -\\alpha \\hat{x}_{n}(1)]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[Z_{n+2}]+\\alpha^{2}\\mathrm{Var}[X_{n+1}-\\hat{x}_{n}(1)]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}(1+\\alpha^{2}).\\nonumber \\end{eqnarray} \\] This process can also be iterated to give \\[\\mathrm{Var}[e_{n}(k)]=\\sigma^{2}_{z}(1+\\alpha^{2}+\\ldots+\\alpha^{2(k-1)})=\\sigma^{2}_{z}\\frac{1-\\alpha^{2k}}{1-\\alpha^{2}} \\] because it is the sum of a geometric progression with finitely many terms. Approximate 95% prediction intervals are now straightforward to calculate as \\[\\hat{x}_{n}(k)\\pm 1.96\\sqrt{\\mathrm{Var}[e_{n}(k)]}. \\] Note: the variance gets a lot higher the further we predict into the future (more uncertainty) 11.4.2 AR(p) forecasting For an AR(p) process \\(X_{t}=\\alpha_{1}X_{t-1}+\\ldots+\\alpha_{p}X_{t-p}+Z_{t}\\), the one step ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(1)&amp;=&amp;\\mathbb{E}[X_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\alpha_{1}X_{n}+\\ldots+\\alpha_{p}X_{n-p+1}+Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha_{1}x_{n}+\\ldots+\\alpha_{p}x_{n-p+1}.\\nonumber \\end{eqnarray} \\] Then for any \\(k\\), the \\(k\\) steps ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(k)&amp;=&amp;\\mathbb{E}[X_{n+k}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\alpha_{1}X_{n+k-1}+\\ldots+\\alpha_{p}X_{n+k-p}+Z_{n+k}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\alpha_{1}\\mathbb{E}[X_{n+k-1}|X_{n},X_{n-1},\\ldots,X_{1}]+\\ldots+\\alpha_{p}\\mathbb{E}[X_{n+k-p}|X_{n},X_{n-1},\\ldots,X_{1}].\\nonumber \\end{eqnarray} \\] Two cases occur for these conditional expectations If \\(X_{n+k-j}\\) has been observed, then \\(\\mathbb{E}[X_{n+k-j}|X_{n},X_{n-1},\\ldots,X_{1}]\\) is equal to its observed value, \\(x_{n+k-j}\\) If \\(X_{n+k-j}\\) is a future value, then \\(\\mathbb{E}[X_{n+k-j}|X_{n},X_{n-1},\\ldots,X_{1}]\\) has already been forecast as one of \\(\\hat{x}_{n}(1),\\ldots,\\hat{x}_{n}(k-1)\\) The error variance for the \\(k\\) steps ahead forecast has the general form \\[\\mathrm{Var}[e_{n}(k)]=\\sigma^{2}_{z}\\sum_{i=0}^{k-1}\\theta_{i}^{2}\\] where \\(\\theta_{0}=1\\), and the remaining \\(\\theta_i\\) are algebraically nasty to determine. Prediction invervals can be calculated using the same formula as for the AR(1) model. 11.4.3 Example 1: Simulated data Simulated AR(1) process of length 100, with 20 future predictions together with prediction intervals n &lt;- 100 x &lt;- arima.sim(model = list(ar = 0.8), n = n) model &lt;- arima(x, order = c(1, 0, 0), include.mean = FALSE) predict.ar1 &lt;- predict(model, n.ahead = 20, se.fit = TRUE) predict.lci &lt;- predict.ar1$pred - 1.96*predict.ar1$se predict.uci &lt;- predict.ar1$pred + 1.96*predict.ar1$se Note how the forecasts fall to zero as \\(k\\) increases from 1 to 20. 11.4.4 Example 2 AR(1) process \\(X_{t}=0.9X_{t-1}+Z_{t}\\), where \\(\\hat{\\sigma}^{2}_{z}=1\\) and \\(x_{n}=20\\). Calculate the one and two steps ahead forecasts and the associated error variances The forecasts are given by \\[\\hat{x}_{n}(1) = \\alpha x_{n} = 0.9\\times 20 = 18 \\] and the error variances are \\[\\mathrm{Var}[e_{n}(1)] = \\hat{\\sigma}^{2}_{z} = 1\\hspace{1cm}\\mbox{and}\\hspace{1cm} \\mathrm{Var}[e_{n}(2)] = \\hat{\\sigma}^{2}_{z}(1+\\alpha^{2}) = 1.81 \\] 11.4.5 Example 3 AR(1) process \\(X_{t}=0.1X_{t-1}+Z_{t}\\) where \\(\\hat\\sigma^2=1\\) and \\(x_n=20\\) Calculate the one and two steps ahead forecasts and the associated error variances The forecasts are given by \\[\\hat{x}_{n}(1) = \\alpha x_{n} = 0.1\\times 20 = 2\\] and the error variances are \\[\\mathrm{Var}[e_{n}(1)] = \\hat{\\sigma}^{2}_{z} = 1\\hspace{1cm}\\mbox{and}\\hspace{1cm} \\mathrm{Var}[e_{n}(2)] = \\hat{\\sigma}^{2}_{z}(1+\\alpha^{2}) = 1.01 \\] Notes: 1. As the lag 1 coefficient gets smaller the forecasts get closer to zero As the lag 1 coefficient gets smaller the two steps ahead forecast error gets smaller 11.5 Forecasting from MA(q) models Forecasting with an MA(q) model is similar to forecasting with an AR(p) model, as both is based on the conditional expectation \\[\\hat{x}_{n}(k)=\\mathbb{E}[X_{n+k}|X_{n},X_{n-1},\\ldots,X_{1}]. \\] 11.5.1 MA(1) forecast For the MA(1) model \\(X_t=\\lambda Z_{t-1}+Z_t\\), the one step ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(1)&amp;=&amp;\\mathbb{E}[X_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\lambda Z_{n}+Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\lambda\\mathbb{E}[Z_{n}|X_{n},X_{n-1},\\ldots,X_{1}] + \\mathbb{E}[Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\lambda z_{n}.\\nonumber \\end{eqnarray} \\] The last line is true because \\(X_{n},X_{n-1},\\ldots,X_{1}\\) do not depend on \\(Z_{n+1}\\), and hence \\[\\mathbb{E}[Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]~=~\\mathbb{E}[Z_{n+1}]~=~0. \\] In contrast, \\(X_n\\) depends on \\(Z_n\\) so \\[\\mathbb{E}[Z_{n}|X_{n},X_{n-1},\\ldots,X_{1}]~\\neq~\\mathbb{E}[Z_{n}]~=~0. \\] \\(Z_n\\) cannot be observed directly but it can be estimated as follows. Re-write the MA(1) process as, \\(Z_t=X_t-\\lambda Z_{t-1}\\) and assuming that \\(Z_0=0\\), \\(Z_t\\) can be estimated iteratively from \\(t=1,\\ldots,n\\) by replacing \\(X_t\\) by its observed value \\(x_t\\) For k&gt;1 the k steps ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(k)&amp;=&amp;\\mathbb{E}[X_{n+k}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\lambda Z_{n+k-1}+Z_{n+k}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;0.\\nonumber \\end{eqnarray} \\] The forecast error variance at one step ahead is given by \\[\\begin{eqnarray} \\mathrm{Var}[e_{n}(1)]&amp;=&amp;\\mathrm{Var}[X_{n+1}-\\hat{x}_{n}(1)]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[\\lambda Z_{n}+Z_{n+1} -\\lambda Z_{n}]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[Z_{n+1}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}\\nonumber \\end{eqnarray} \\] while for k&gt;1 it is given by \\[\\begin{eqnarray} \\mathrm{Var}[e_{n}(k)]&amp;=&amp;\\mathrm{Var}[X_{n+k}-\\hat{x}_{n}(k)]\\nonumber\\\\ &amp;=&amp;\\mathrm{Var}[X_{n+k}]\\nonumber\\\\ &amp;=&amp;\\sigma^{2}_{z}(1+\\lambda^2).\\nonumber \\end{eqnarray} \\] Then 95% prediction intervals can be calculated as before using the formula \\[\\hat{x}_{n}(k)\\pm 1.96\\sqrt{\\mathrm{Var}[e_{n}(k)]}. \\] 11.5.2 MA(q) forecasts Works the same way as MA(1) model, the one step ahead forecast is given by \\[\\begin{eqnarray} \\hat{x}_{n}(1)&amp;=&amp;\\mathbb{E}[X_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\mathbb{E}[\\lambda_{1}Z_{n}+\\ldots+\\lambda_{q}Z_{n-q+1}+Z_{n+1}|X_{n},X_{n-1},\\ldots,X_{1}]\\nonumber\\\\ &amp;=&amp;\\lambda_{1}z_{n}+\\ldots+\\lambda_{q}z_{n-q+1}\\nonumber \\end{eqnarray} \\] where as before the current and past values of \\(Z_t\\) are calculated recursively from the MA(q) equation \\[Z_{t}=X_{t}-\\lambda_{1}Z_{t-1}-\\ldots-\\lambda_{q}Z_{t-q}\\] with the initial conditions \\(Z_{0}=Z_{-1}=\\ldots=Z_{-q+1}=0\\). The general k steps ahead forecast is calculated in an identical way, where current past values of \\(Z_t\\) are estimate from the data, while future values are set to zero. Therefore the forecast is given by \\[\\begin{equation*} \\hat{x}_n(k) = \\begin{cases} \\lambda_k z_n + \\cdots + \\lambda_q z_{n+k-q} &amp; \\text{if $k\\leq q$} \\\\ 0 &amp; \\text{if $k &gt; q$}. \\end{cases} \\end{equation*} \\] It is straightforward to show that the k steps ahead error variance is given by \\[\\begin{equation*} e_n(k) = \\begin{cases} \\sigma^{2}_{z}[1+\\sum_{i=1}^{k-1}\\lambda_{i}^{2}]&amp; \\text{if $k\\leq q$} \\\\ \\sigma^{2}_{z}[1+\\sum_{i=1}^{q}\\lambda_{i}^{2}] &amp; \\text{if $k &gt; q$}. \\end{cases} \\end{equation*} \\] 11.5.2.1 Example 1 MA(3) process of length 100 with 20 future predictions together with prediction intervals. Note how the forecasts fall to zero for k &gt; 3, which was shown algebraically earlier. 11.5.2.2 Example 2 Consider modelling the short time series \\(\\mathbf{x}=(3,8,2,5,6)\\) with an MA(1) time series process \\(X_{t}=0.7Z_{t-1}+Z_{t}\\) where \\(\\hat{\\sigma}^{2}_{z}=1\\). Calculate the one and two steps ahead forecasts \\(x_{5}(1)\\) and \\(x_{5}(2)\\) as well as their associated error variances To calculate the forecasts we need to recursively estimate \\(Z_{1},\\ldots,Z_{5}\\) assuming that \\(z_{0}=0\\). This gives \\[z_{1}=3,\\hspace{0.5cm}z_{2}=5.9,\\hspace{0.5cm}z_{3}=-2.13, \\hspace{0.5cm}z_{4}=6.491, \\hspace{0.5cm}z_{5}=1.4563 \\] \\[\\hat{x}_{5}(1)~=~\\lambda z_{5}~=~0.7\\times 1.4563~=~1.01941 \\] \\[\\hat{x}_{n}(2)~=~0 \\] And the error variances are \\[\\mathrm{Var}[e_{n}(1)]~=~\\hat{\\sigma}^{2}_{z}~=~1\\hspace{1cm}\\mbox{and}\\hspace{1cm} \\mathrm{Var}[e_{n}(2)]~=~\\hat{\\sigma}^{2}_{z}(1+\\lambda^2)~=~1.49. \\] 11.6 Forecasting time series with trend, seasonality and correlation There are a number of ways to forecast a time series that contains trend and seasonal variation in addition to short-term correlation. The method we consider in this course is a natural combination of regression and ARMA(p,q) models. For the time series model \\(X_t=m_t+s_t+e_t\\) we represent the trend and seasonal variation by \\(m_t+s_t=\\textbf{z}_{t}^\\top \\boldsymbol{\\beta}\\) while the residuals are given by \\(e_{t}^{*}=X_{t}-\\textbf{z}_{t}^\\top\\hat{\\boldsymbol{\\beta}}\\) and are modelled by a stationary ARMA(p,q) process. Then the k steps ahead forecast is given by \\[\\hat{x}_{n}(k)=\\textbf{z}_{n+1}^\\top\\hat{\\boldsymbol{\\beta}} + e_{n}^{*}(k) \\] Where the stationary process \\(e_{n}^{*}(k)\\) is prediced using an AR(p), MA(q) or ARMA(p,q) model 11.6.1 Example This figure shows a simulated AR(1) process with a linear trend of length 100 with 20 future predictions together n &lt;- 100 time &lt;- 1:n time.predict &lt;- (n + 1):120 x &lt;- arima.sim(model = list(ar = 0.8), n = n) + 30 + 0.1*time model.ar1 &lt;- arima(x, order = c(1, 0, 0), xreg = time, include.mean = TRUE) predict.ar1 &lt;- predict(model.ar1, n.ahead = 20, newxreg = time.predict, se.fit = TRUE) ar1.LCI &lt;- predict.ar1$pred - 1.96*predict.ar1$se ar1.UCI &lt;- predict.ar1$pred + 1.96*predict.ar1$se "],
["chapter-6-lab.html", "12 Chapter 6 Lab 12.1 Forecasting with ARIMA 12.2 Global Temp data 12.3 Exponential smoothing", " 12 Chapter 6 Lab 12.1 Forecasting with ARIMA library(astsa) # Plot P/ACF pair of differenced data acf2(diff(x)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] ## ACF -0.12 0.00 -0.07 0.02 -0.06 -0.21 0.05 -0.09 0.09 -0.05 -0.12 0.12 ## PACF -0.12 -0.02 -0.07 0.01 -0.05 -0.23 -0.01 -0.11 0.04 -0.04 -0.19 0.04 ## [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] ## ACF -0.09 -0.1 0.00 0.00 0.01 -0.12 0.13 -0.05 ## PACF -0.11 -0.2 -0.01 -0.11 -0.09 -0.17 -0.04 -0.12 # Fit model - check t-table and diagnostics sarima(x, 1, 1, 0) ## initial value 0.137947 ## iter 2 value 0.130725 ## iter 3 value 0.130723 ## iter 4 value 0.130723 ## iter 4 value 0.130723 ## iter 4 value 0.130723 ## final value 0.130723 ## converged ## initial value 0.128980 ## iter 2 value 0.128956 ## iter 3 value 0.128954 ## iter 3 value 0.128954 ## iter 3 value 0.128954 ## final value 0.128954 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ar1 constant ## -0.1192 0.0788 ## s.e. 0.1000 0.1023 ## ## sigma^2 estimated as 1.294: log likelihood = -153.24, aic = 312.48 ## ## $degrees_of_freedom ## [1] 97 ## ## $ttable ## Estimate SE t.value p.value ## ar1 -0.1192 0.1000 -1.1921 0.2361 ## constant 0.0788 0.1023 0.7707 0.4428 ## ## $AIC ## [1] 3.156391 ## ## $AICc ## [1] 3.157653 ## ## $BIC ## [1] 3.235031 # Forecast the data 20 time periods ahead sarima.for(x, n.ahead = 20, p = 1, d = 1, q = 0) ## $pred ## Time Series: ## Start = 101 ## End = 120 ## Frequency = 1 ## [1] 37.93860 38.03106 38.10825 38.18726 38.26606 38.34488 38.42369 38.50251 ## [9] 38.58133 38.66015 38.73897 38.81778 38.89660 38.97542 39.05424 39.13306 ## [17] 39.21187 39.29069 39.36951 39.44833 ## ## $se ## Time Series: ## Start = 101 ## End = 120 ## Frequency = 1 ## [1] 1.137555 1.515934 1.826118 2.089843 2.323929 2.536493 2.732572 2.915493 ## [9] 3.087597 3.250601 3.405813 3.554253 3.696737 3.833930 3.966381 4.094549 ## [17] 4.218825 4.339543 4.456993 4.571427 12.2 Global Temp data head(globtemp) ## [1] -0.20 -0.11 -0.10 -0.20 -0.28 -0.31 plot(globtemp) # Fit an ARIMA(0,1,2) to globtemp and check the fit sarima(globtemp, 0,1,2) ## initial value -2.220513 ## iter 2 value -2.294887 ## iter 3 value -2.307682 ## iter 4 value -2.309170 ## iter 5 value -2.310360 ## iter 6 value -2.311251 ## iter 7 value -2.311636 ## iter 8 value -2.311648 ## iter 9 value -2.311649 ## iter 9 value -2.311649 ## iter 9 value -2.311649 ## final value -2.311649 ## converged ## initial value -2.310187 ## iter 2 value -2.310197 ## iter 3 value -2.310199 ## iter 4 value -2.310201 ## iter 5 value -2.310202 ## iter 5 value -2.310202 ## iter 5 value -2.310202 ## final value -2.310202 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = constant, transform.pars = trans, fixed = fixed, ## optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 ma2 constant ## -0.3984 -0.2173 0.0072 ## s.e. 0.0808 0.0768 0.0033 ## ## sigma^2 estimated as 0.00982: log likelihood = 120.32, aic = -232.64 ## ## $degrees_of_freedom ## [1] 132 ## ## $ttable ## Estimate SE t.value p.value ## ma1 -0.3984 0.0808 -4.9313 0.0000 ## ma2 -0.2173 0.0768 -2.8303 0.0054 ## constant 0.0072 0.0033 2.1463 0.0337 ## ## $AIC ## [1] -1.723268 ## ## $AICc ## [1] -1.721911 ## ## $BIC ## [1] -1.637185 # Forecast data 35 years into the future sarima.for(globtemp, 35, 0,1,2) ## $pred ## Time Series: ## Start = 2016 ## End = 2050 ## Frequency = 1 ## [1] 0.7995567 0.7745381 0.7816919 0.7888457 0.7959996 0.8031534 0.8103072 ## [8] 0.8174611 0.8246149 0.8317688 0.8389226 0.8460764 0.8532303 0.8603841 ## [15] 0.8675379 0.8746918 0.8818456 0.8889995 0.8961533 0.9033071 0.9104610 ## [22] 0.9176148 0.9247687 0.9319225 0.9390763 0.9462302 0.9533840 0.9605378 ## [29] 0.9676917 0.9748455 0.9819994 0.9891532 0.9963070 1.0034609 1.0106147 ## ## $se ## Time Series: ## Start = 2016 ## End = 2050 ## Frequency = 1 ## [1] 0.09909556 0.11564576 0.12175580 0.12757353 0.13313729 0.13847769 ## [7] 0.14361964 0.14858376 0.15338730 0.15804492 0.16256915 0.16697084 ## [13] 0.17125943 0.17544322 0.17952954 0.18352490 0.18743511 0.19126540 ## [19] 0.19502047 0.19870459 0.20232164 0.20587515 0.20936836 0.21280424 ## [25] 0.21618551 0.21951471 0.22279416 0.22602604 0.22921235 0.23235497 ## [31] 0.23545565 0.23851603 0.24153763 0.24452190 0.24747019 Pure seasonal model takes 4 more parameters P, S, D, Q # Plot sample P/ACF to lag 60 and compare to the true values acf2(x, max.lag = 60) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## ACF 0.9 0.82 0.73 0.64 0.56 0.50 0.48 0.45 0.44 0.42 0.40 0.4 0.37 ## PACF 0.9 0.04 -0.07 -0.04 -0.02 0.05 0.15 0.03 0.03 -0.07 0.04 0.1 -0.09 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## ACF 0.35 0.36 0.36 0.37 0.36 0.36 0.34 0.34 0.32 0.29 0.27 0.20 ## PACF 0.05 0.13 0.04 0.02 -0.08 0.06 -0.06 0.09 -0.02 -0.11 -0.01 -0.19 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## ACF 0.16 0.12 0.10 0.07 0.03 0.02 0.00 -0.03 -0.06 -0.08 -0.08 -0.09 ## PACF 0.02 0.04 0.02 -0.11 -0.10 0.08 -0.04 -0.11 -0.07 0.07 0.02 0.02 ## [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## ACF -0.09 -0.07 -0.06 -0.05 -0.07 -0.06 -0.07 -0.08 -0.08 -0.07 -0.07 -0.07 ## PACF 0.01 0.00 -0.02 0.03 0.00 0.06 -0.04 -0.02 0.07 0.15 -0.10 -0.04 ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] ## ACF -0.08 -0.11 -0.13 -0.17 -0.21 -0.24 -0.29 -0.32 -0.35 -0.34 -0.32 ## PACF -0.05 -0.03 -0.02 -0.02 -0.15 -0.06 -0.13 0.00 0.01 0.05 -0.05 # Fit the seasonal model to x sarima(x, p = 0, d = 0, q = 0, P = 1, D = 0, Q = 1, S = 12) ## initial value 0.958236 ## iter 2 value 0.876547 ## iter 3 value 0.812976 ## iter 4 value 0.777142 ## iter 5 value 0.747937 ## iter 6 value 0.725263 ## iter 7 value 0.719720 ## iter 8 value 0.693755 ## iter 9 value 0.690447 ## iter 10 value 0.681477 ## iter 11 value 0.680611 ## iter 12 value 0.680126 ## iter 13 value 0.679664 ## iter 14 value 0.678787 ## iter 15 value 0.678244 ## iter 16 value 0.678215 ## iter 17 value 0.678182 ## iter 18 value 0.678167 ## iter 19 value 0.678158 ## iter 20 value 0.678128 ## iter 21 value 0.678102 ## iter 22 value 0.678085 ## iter 23 value 0.678084 ## iter 24 value 0.678084 ## iter 25 value 0.678083 ## iter 26 value 0.678083 ## iter 27 value 0.678083 ## iter 28 value 0.678083 ## iter 29 value 0.678083 ## iter 29 value 0.678083 ## final value 0.678083 ## converged ## initial value 1.316632 ## iter 2 value 1.078864 ## iter 3 value 1.008425 ## iter 4 value 0.990485 ## iter 5 value 0.972791 ## iter 6 value 0.957820 ## iter 7 value 0.946498 ## iter 8 value 0.936672 ## iter 9 value 0.929951 ## iter 10 value 0.915219 ## iter 11 value 0.913770 ## iter 12 value 0.904973 ## iter 13 value 0.903657 ## iter 14 value 0.903549 ## iter 15 value 0.903545 ## iter 16 value 0.903544 ## iter 17 value 0.903543 ## iter 18 value 0.903542 ## iter 19 value 0.903542 ## iter 19 value 0.903542 ## iter 19 value 0.903542 ## final value 0.903542 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## sar1 sma1 xmean ## 0.7141 -0.2117 34.4740 ## s.e. 0.1099 0.1181 0.5357 ## ## sigma^2 estimated as 5.785: log likelihood = -232.25, aic = 472.5 ## ## $degrees_of_freedom ## [1] 97 ## ## $ttable ## Estimate SE t.value p.value ## sar1 0.7141 0.1099 6.4973 0.0000 ## sma1 -0.2117 0.1181 -1.7927 0.0761 ## xmean 34.4740 0.5357 64.3483 0.0000 ## ## $AIC ## [1] 4.724962 ## ## $AICc ## [1] 4.727462 ## ## $BIC ## [1] 4.829169 However, pure seasonal won’t be likely. Data in real life will tend to be mixed seasonal model (specified p, d, q in addition to P, D, Q, S) # Plot sample P/ACF pair to lag 60 and compare to actual acf2(x, max.lag = 60) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## ACF 0.9 0.82 0.73 0.64 0.56 0.50 0.48 0.45 0.44 0.42 0.40 0.4 0.37 ## PACF 0.9 0.04 -0.07 -0.04 -0.02 0.05 0.15 0.03 0.03 -0.07 0.04 0.1 -0.09 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] ## ACF 0.35 0.36 0.36 0.37 0.36 0.36 0.34 0.34 0.32 0.29 0.27 0.20 ## PACF 0.05 0.13 0.04 0.02 -0.08 0.06 -0.06 0.09 -0.02 -0.11 -0.01 -0.19 ## [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] ## ACF 0.16 0.12 0.10 0.07 0.03 0.02 0.00 -0.03 -0.06 -0.08 -0.08 -0.09 ## PACF 0.02 0.04 0.02 -0.11 -0.10 0.08 -0.04 -0.11 -0.07 0.07 0.02 0.02 ## [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] ## ACF -0.09 -0.07 -0.06 -0.05 -0.07 -0.06 -0.07 -0.08 -0.08 -0.07 -0.07 -0.07 ## PACF 0.01 0.00 -0.02 0.03 0.00 0.06 -0.04 -0.02 0.07 0.15 -0.10 -0.04 ## [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] ## ACF -0.08 -0.11 -0.13 -0.17 -0.21 -0.24 -0.29 -0.32 -0.35 -0.34 -0.32 ## PACF -0.05 -0.03 -0.02 -0.02 -0.15 -0.06 -0.13 0.00 0.01 0.05 -0.05 # Fit the seasonal model to x sarima(x, p = 0, d = 0, q = 1, P = 0, D = 0, Q = 1, S = 12) ## initial value 1.040726 ## iter 2 value 0.685896 ## iter 3 value 0.666207 ## iter 4 value 0.607557 ## iter 5 value 0.602804 ## iter 6 value 0.596031 ## iter 7 value 0.595761 ## iter 8 value 0.595755 ## iter 9 value 0.595754 ## iter 10 value 0.595754 ## iter 11 value 0.595754 ## iter 12 value 0.595754 ## iter 12 value 0.595754 ## iter 12 value 0.595754 ## final value 0.595754 ## converged ## initial value 0.588139 ## iter 2 value 0.587695 ## iter 3 value 0.587540 ## iter 4 value 0.587468 ## iter 5 value 0.587440 ## iter 6 value 0.587440 ## iter 7 value 0.587440 ## iter 7 value 0.587440 ## iter 7 value 0.587440 ## final value 0.587440 ## converged ## $fit ## ## Call: ## stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, ## Q), period = S), xreg = xmean, include.mean = FALSE, transform.pars = trans, ## fixed = fixed, optim.control = list(trace = trc, REPORT = 1, reltol = tol)) ## ## Coefficients: ## ma1 sma1 xmean ## 0.7270 0.2512 34.4628 ## s.e. 0.0588 0.0803 0.3758 ## ## sigma^2 estimated as 3.189: log likelihood = -200.64, aic = 409.28 ## ## $degrees_of_freedom ## [1] 97 ## ## $ttable ## Estimate SE t.value p.value ## ma1 0.7270 0.0588 12.3579 0.0000 ## sma1 0.2512 0.0803 3.1284 0.0023 ## xmean 34.4628 0.3758 91.7171 0.0000 ## ## $AIC ## [1] 4.092756 ## ## $AICc ## [1] 4.095256 ## ## $BIC ## [1] 4.196963 12.3 Exponential smoothing library(forecast) fc &lt;- ses(birth, h = 10) summary(fc) ## ## Forecast method: Simple exponential smoothing ## ## Model Information: ## Simple exponential smoothing ## ## Call: ## ses(y = birth, h = 10) ## ## Smoothing parameters: ## alpha = 0.7106 ## ## Initial states: ## l = 292.9802 ## ## sigma: 16.2158 ## ## AIC AICc BIC ## 4291.087 4291.152 4302.851 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.0570963 16.17222 13.034 -0.195208 4.228966 1.328423 -0.02023071 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Feb 1979 277.8466 257.0653 298.6279 246.0643 309.6289 ## Mar 1979 277.8466 252.3528 303.3404 238.8572 316.8360 ## Apr 1979 277.8466 248.3847 307.3085 232.7885 322.9047 ## May 1979 277.8466 244.8910 310.8022 227.4453 328.2479 ## Jun 1979 277.8466 241.7337 313.9595 222.6167 333.0765 ## Jul 1979 277.8466 238.8311 316.8621 218.1775 337.5157 ## Aug 1979 277.8466 236.1299 319.5633 214.0464 341.6468 ## Sep 1979 277.8466 233.5933 322.0999 210.1671 345.5261 ## Oct 1979 277.8466 231.1945 324.4987 206.4983 349.1949 ## Nov 1979 277.8466 228.9131 326.7801 203.0092 352.6840 autoplot(fc) # Add the one-step forecasts for the training data to the plot autoplot(fc) + autolayer(fitted(fc)) library(quantmod) ## Loading required package: xts ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: TTR ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## Version 0.4-0 included new data defaults. See ?getSymbols. getSymbols(&quot;CPIAUCSL&quot;, auto.assign = TRUE, src = &quot;FRED&quot;) ## &#39;getSymbols&#39; currently uses auto.assign=TRUE by default, but will ## use auto.assign=FALSE in 0.5-0. You will still be able to use ## &#39;loadSymbols&#39; to automatically load data. getOption(&quot;getSymbols.env&quot;) ## and getOption(&quot;getSymbols.auto.assign&quot;) will still be checked for ## alternate defaults. ## ## This message is shown once per session and may be disabled by setting ## options(&quot;getSymbols.warning4.0&quot;=FALSE). See ?getSymbols for details. ## [1] &quot;CPIAUCSL&quot; getSymbols(&quot;USSTHPI&quot;, auto.assign = TRUE, src = &quot;FRED&quot;) ## [1] &quot;USSTHPI&quot; CPI &lt;- read.csv(&quot;data/CPIAUCSL.csv&quot;) USHousePriceIndex &lt;- read.csv(&quot;data/USSTHPI.csv&quot;) plot(CPIAUCSL) plot(USSTHPI) "],
["references.html", "References", " References Time Series Fundamentals https://bookdown.org/gary_a_napier/time_series_lecture_notes/ChapterOne.html "]
]
